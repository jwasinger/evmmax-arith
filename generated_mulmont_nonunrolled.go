package evmmax_arith

import (
	"errors"
	"fmt"
	"math/bits"
	"unsafe"
)

func MulMontNonUnrolled64(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[1]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[1]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[1]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[1]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [2]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])

	t[1], D = bits.Add64(t[1], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	t[0], C = bits.Add64(t[1], C, 0)
	t[1], _ = bits.Add64(0, D, C)

	for j := 1; j < 1; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		t[1], D = bits.Add64(t[1], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		t[0], C = bits.Add64(t[1], C, 0)
		t[1], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)

	if D != 0 && t[1] == 0 {
		// reduction was not necessary
		copy(z[:], t[:1])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled128(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[2]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[2]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[2]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[2]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [3]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)

	t[2], D = bits.Add64(t[2], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	t[1], C = bits.Add64(t[2], C, 0)
	t[2], _ = bits.Add64(0, D, C)

	for j := 1; j < 2; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		t[2], D = bits.Add64(t[2], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		t[1], C = bits.Add64(t[2], C, 0)
		t[2], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)

	if D != 0 && t[2] == 0 {
		// reduction was not necessary
		copy(z[:], t[:2])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled192(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[3]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[3]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[3]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[3]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [4]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)

	t[3], D = bits.Add64(t[3], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	t[2], C = bits.Add64(t[3], C, 0)
	t[3], _ = bits.Add64(0, D, C)

	for j := 1; j < 3; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		t[3], D = bits.Add64(t[3], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		t[2], C = bits.Add64(t[3], C, 0)
		t[3], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)

	if D != 0 && t[3] == 0 {
		// reduction was not necessary
		copy(z[:], t[:3])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled256(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[4]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[4]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[4]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[4]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [5]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)

	t[4], D = bits.Add64(t[4], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	t[3], C = bits.Add64(t[4], C, 0)
	t[4], _ = bits.Add64(0, D, C)

	for j := 1; j < 4; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		t[4], D = bits.Add64(t[4], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		t[3], C = bits.Add64(t[4], C, 0)
		t[4], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)

	if D != 0 && t[4] == 0 {
		// reduction was not necessary
		copy(z[:], t[:4])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled320(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[5]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[5]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[5]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[5]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [6]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)

	t[5], D = bits.Add64(t[5], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	t[4], C = bits.Add64(t[5], C, 0)
	t[5], _ = bits.Add64(0, D, C)

	for j := 1; j < 5; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		t[5], D = bits.Add64(t[5], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		t[4], C = bits.Add64(t[5], C, 0)
		t[5], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)

	if D != 0 && t[5] == 0 {
		// reduction was not necessary
		copy(z[:], t[:5])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled384(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[6]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[6]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[6]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[6]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [7]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)

	t[6], D = bits.Add64(t[6], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	t[5], C = bits.Add64(t[6], C, 0)
	t[6], _ = bits.Add64(0, D, C)

	for j := 1; j < 6; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		t[6], D = bits.Add64(t[6], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		t[5], C = bits.Add64(t[6], C, 0)
		t[6], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)

	if D != 0 && t[6] == 0 {
		// reduction was not necessary
		copy(z[:], t[:6])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled448(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[7]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[7]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[7]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[7]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [8]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)

	t[7], D = bits.Add64(t[7], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	t[6], C = bits.Add64(t[7], C, 0)
	t[7], _ = bits.Add64(0, D, C)

	for j := 1; j < 7; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		t[7], D = bits.Add64(t[7], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		t[6], C = bits.Add64(t[7], C, 0)
		t[7], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)

	if D != 0 && t[7] == 0 {
		// reduction was not necessary
		copy(z[:], t[:7])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled512(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[8]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[8]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[8]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[8]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [9]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)

	t[8], D = bits.Add64(t[8], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	t[7], C = bits.Add64(t[8], C, 0)
	t[8], _ = bits.Add64(0, D, C)

	for j := 1; j < 8; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		t[8], D = bits.Add64(t[8], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		t[7], C = bits.Add64(t[8], C, 0)
		t[8], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)

	if D != 0 && t[8] == 0 {
		// reduction was not necessary
		copy(z[:], t[:8])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled576(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[9]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[9]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[9]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[9]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [10]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)

	t[9], D = bits.Add64(t[9], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	t[8], C = bits.Add64(t[9], C, 0)
	t[9], _ = bits.Add64(0, D, C)

	for j := 1; j < 9; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		t[9], D = bits.Add64(t[9], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		t[8], C = bits.Add64(t[9], C, 0)
		t[9], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)

	if D != 0 && t[9] == 0 {
		// reduction was not necessary
		copy(z[:], t[:9])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled640(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[10]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[10]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[10]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[10]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [11]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)

	t[10], D = bits.Add64(t[10], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	t[9], C = bits.Add64(t[10], C, 0)
	t[10], _ = bits.Add64(0, D, C)

	for j := 1; j < 10; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		t[10], D = bits.Add64(t[10], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		t[9], C = bits.Add64(t[10], C, 0)
		t[10], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)

	if D != 0 && t[10] == 0 {
		// reduction was not necessary
		copy(z[:], t[:10])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled704(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[11]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[11]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[11]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[11]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [12]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)

	t[11], D = bits.Add64(t[11], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	t[10], C = bits.Add64(t[11], C, 0)
	t[11], _ = bits.Add64(0, D, C)

	for j := 1; j < 11; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		t[11], D = bits.Add64(t[11], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		t[10], C = bits.Add64(t[11], C, 0)
		t[11], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)

	if D != 0 && t[11] == 0 {
		// reduction was not necessary
		copy(z[:], t[:11])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled768(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[12]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[12]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[12]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[12]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [13]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)

	t[12], D = bits.Add64(t[12], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	t[11], C = bits.Add64(t[12], C, 0)
	t[12], _ = bits.Add64(0, D, C)

	for j := 1; j < 12; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		t[12], D = bits.Add64(t[12], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		t[11], C = bits.Add64(t[12], C, 0)
		t[12], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)

	if D != 0 && t[12] == 0 {
		// reduction was not necessary
		copy(z[:], t[:12])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled832(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[13]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[13]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[13]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[13]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [14]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)

	t[13], D = bits.Add64(t[13], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	t[12], C = bits.Add64(t[13], C, 0)
	t[13], _ = bits.Add64(0, D, C)

	for j := 1; j < 13; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		t[13], D = bits.Add64(t[13], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		t[12], C = bits.Add64(t[13], C, 0)
		t[13], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)

	if D != 0 && t[13] == 0 {
		// reduction was not necessary
		copy(z[:], t[:13])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled896(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[14]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[14]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[14]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[14]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [15]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)

	t[14], D = bits.Add64(t[14], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	t[13], C = bits.Add64(t[14], C, 0)
	t[14], _ = bits.Add64(0, D, C)

	for j := 1; j < 14; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		t[14], D = bits.Add64(t[14], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		t[13], C = bits.Add64(t[14], C, 0)
		t[14], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)

	if D != 0 && t[14] == 0 {
		// reduction was not necessary
		copy(z[:], t[:14])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled960(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[15]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[15]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[15]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[15]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [16]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)

	t[15], D = bits.Add64(t[15], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	t[14], C = bits.Add64(t[15], C, 0)
	t[15], _ = bits.Add64(0, D, C)

	for j := 1; j < 15; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		t[15], D = bits.Add64(t[15], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		t[14], C = bits.Add64(t[15], C, 0)
		t[15], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)

	if D != 0 && t[15] == 0 {
		// reduction was not necessary
		copy(z[:], t[:15])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled1024(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[16]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[16]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[16]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[16]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [17]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)

	t[16], D = bits.Add64(t[16], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	t[15], C = bits.Add64(t[16], C, 0)
	t[16], _ = bits.Add64(0, D, C)

	for j := 1; j < 16; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		t[16], D = bits.Add64(t[16], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		t[15], C = bits.Add64(t[16], C, 0)
		t[16], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)

	if D != 0 && t[16] == 0 {
		// reduction was not necessary
		copy(z[:], t[:16])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled1088(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[17]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[17]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[17]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[17]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [18]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)

	t[17], D = bits.Add64(t[17], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	t[16], C = bits.Add64(t[17], C, 0)
	t[17], _ = bits.Add64(0, D, C)

	for j := 1; j < 17; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		t[17], D = bits.Add64(t[17], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		t[16], C = bits.Add64(t[17], C, 0)
		t[17], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)

	if D != 0 && t[17] == 0 {
		// reduction was not necessary
		copy(z[:], t[:17])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled1152(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[18]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[18]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[18]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[18]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [19]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)

	t[18], D = bits.Add64(t[18], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	t[17], C = bits.Add64(t[18], C, 0)
	t[18], _ = bits.Add64(0, D, C)

	for j := 1; j < 18; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		t[18], D = bits.Add64(t[18], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		t[17], C = bits.Add64(t[18], C, 0)
		t[18], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)

	if D != 0 && t[18] == 0 {
		// reduction was not necessary
		copy(z[:], t[:18])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled1216(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[19]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[19]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[19]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[19]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [20]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)

	t[19], D = bits.Add64(t[19], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	t[18], C = bits.Add64(t[19], C, 0)
	t[19], _ = bits.Add64(0, D, C)

	for j := 1; j < 19; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		t[19], D = bits.Add64(t[19], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		t[18], C = bits.Add64(t[19], C, 0)
		t[19], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)

	if D != 0 && t[19] == 0 {
		// reduction was not necessary
		copy(z[:], t[:19])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled1280(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[20]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[20]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[20]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[20]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [21]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)

	t[20], D = bits.Add64(t[20], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	t[19], C = bits.Add64(t[20], C, 0)
	t[20], _ = bits.Add64(0, D, C)

	for j := 1; j < 20; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		t[20], D = bits.Add64(t[20], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		t[19], C = bits.Add64(t[20], C, 0)
		t[20], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)

	if D != 0 && t[20] == 0 {
		// reduction was not necessary
		copy(z[:], t[:20])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled1344(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[21]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[21]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[21]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[21]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [22]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)

	t[21], D = bits.Add64(t[21], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	t[20], C = bits.Add64(t[21], C, 0)
	t[21], _ = bits.Add64(0, D, C)

	for j := 1; j < 21; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		t[21], D = bits.Add64(t[21], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		t[20], C = bits.Add64(t[21], C, 0)
		t[21], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)

	if D != 0 && t[21] == 0 {
		// reduction was not necessary
		copy(z[:], t[:21])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled1408(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[22]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[22]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[22]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[22]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [23]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)

	t[22], D = bits.Add64(t[22], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	t[21], C = bits.Add64(t[22], C, 0)
	t[22], _ = bits.Add64(0, D, C)

	for j := 1; j < 22; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		t[22], D = bits.Add64(t[22], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		t[21], C = bits.Add64(t[22], C, 0)
		t[22], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)

	if D != 0 && t[22] == 0 {
		// reduction was not necessary
		copy(z[:], t[:22])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled1472(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[23]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[23]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[23]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[23]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [24]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)

	t[23], D = bits.Add64(t[23], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	t[22], C = bits.Add64(t[23], C, 0)
	t[23], _ = bits.Add64(0, D, C)

	for j := 1; j < 23; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		t[23], D = bits.Add64(t[23], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		t[22], C = bits.Add64(t[23], C, 0)
		t[23], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)

	if D != 0 && t[23] == 0 {
		// reduction was not necessary
		copy(z[:], t[:23])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled1536(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[24]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[24]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[24]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[24]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [25]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)

	t[24], D = bits.Add64(t[24], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	t[23], C = bits.Add64(t[24], C, 0)
	t[24], _ = bits.Add64(0, D, C)

	for j := 1; j < 24; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		t[24], D = bits.Add64(t[24], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		t[23], C = bits.Add64(t[24], C, 0)
		t[24], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)

	if D != 0 && t[24] == 0 {
		// reduction was not necessary
		copy(z[:], t[:24])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled1600(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[25]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[25]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[25]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[25]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [26]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)

	t[25], D = bits.Add64(t[25], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	t[24], C = bits.Add64(t[25], C, 0)
	t[25], _ = bits.Add64(0, D, C)

	for j := 1; j < 25; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		t[25], D = bits.Add64(t[25], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		t[24], C = bits.Add64(t[25], C, 0)
		t[25], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)

	if D != 0 && t[25] == 0 {
		// reduction was not necessary
		copy(z[:], t[:25])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled1664(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[26]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[26]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[26]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[26]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [27]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)

	t[26], D = bits.Add64(t[26], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	t[25], C = bits.Add64(t[26], C, 0)
	t[26], _ = bits.Add64(0, D, C)

	for j := 1; j < 26; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		t[26], D = bits.Add64(t[26], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		t[25], C = bits.Add64(t[26], C, 0)
		t[26], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)

	if D != 0 && t[26] == 0 {
		// reduction was not necessary
		copy(z[:], t[:26])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled1728(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[27]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[27]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[27]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[27]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [28]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)

	t[27], D = bits.Add64(t[27], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	t[26], C = bits.Add64(t[27], C, 0)
	t[27], _ = bits.Add64(0, D, C)

	for j := 1; j < 27; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		t[27], D = bits.Add64(t[27], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		t[26], C = bits.Add64(t[27], C, 0)
		t[27], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)

	if D != 0 && t[27] == 0 {
		// reduction was not necessary
		copy(z[:], t[:27])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled1792(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[28]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[28]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[28]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[28]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [29]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)

	t[28], D = bits.Add64(t[28], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	t[27], C = bits.Add64(t[28], C, 0)
	t[28], _ = bits.Add64(0, D, C)

	for j := 1; j < 28; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		t[28], D = bits.Add64(t[28], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		t[27], C = bits.Add64(t[28], C, 0)
		t[28], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)

	if D != 0 && t[28] == 0 {
		// reduction was not necessary
		copy(z[:], t[:28])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled1856(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[29]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[29]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[29]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[29]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [30]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)

	t[29], D = bits.Add64(t[29], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	t[28], C = bits.Add64(t[29], C, 0)
	t[29], _ = bits.Add64(0, D, C)

	for j := 1; j < 29; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		t[29], D = bits.Add64(t[29], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		t[28], C = bits.Add64(t[29], C, 0)
		t[29], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)

	if D != 0 && t[29] == 0 {
		// reduction was not necessary
		copy(z[:], t[:29])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled1920(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[30]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[30]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[30]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[30]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [31]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)

	t[30], D = bits.Add64(t[30], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	t[29], C = bits.Add64(t[30], C, 0)
	t[30], _ = bits.Add64(0, D, C)

	for j := 1; j < 30; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		t[30], D = bits.Add64(t[30], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		t[29], C = bits.Add64(t[30], C, 0)
		t[30], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)

	if D != 0 && t[30] == 0 {
		// reduction was not necessary
		copy(z[:], t[:30])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled1984(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[31]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[31]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[31]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[31]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [32]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)

	t[31], D = bits.Add64(t[31], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	t[30], C = bits.Add64(t[31], C, 0)
	t[31], _ = bits.Add64(0, D, C)

	for j := 1; j < 31; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		t[31], D = bits.Add64(t[31], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		t[30], C = bits.Add64(t[31], C, 0)
		t[31], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)

	if D != 0 && t[31] == 0 {
		// reduction was not necessary
		copy(z[:], t[:31])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled2048(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[32]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[32]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[32]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[32]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [33]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)

	t[32], D = bits.Add64(t[32], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	t[31], C = bits.Add64(t[32], C, 0)
	t[32], _ = bits.Add64(0, D, C)

	for j := 1; j < 32; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		t[32], D = bits.Add64(t[32], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		t[31], C = bits.Add64(t[32], C, 0)
		t[32], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)

	if D != 0 && t[32] == 0 {
		// reduction was not necessary
		copy(z[:], t[:32])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled2112(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[33]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[33]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[33]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[33]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [34]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)

	t[33], D = bits.Add64(t[33], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	t[32], C = bits.Add64(t[33], C, 0)
	t[33], _ = bits.Add64(0, D, C)

	for j := 1; j < 33; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		t[33], D = bits.Add64(t[33], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		t[32], C = bits.Add64(t[33], C, 0)
		t[33], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)

	if D != 0 && t[33] == 0 {
		// reduction was not necessary
		copy(z[:], t[:33])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled2176(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[34]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[34]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[34]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[34]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [35]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)

	t[34], D = bits.Add64(t[34], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	t[33], C = bits.Add64(t[34], C, 0)
	t[34], _ = bits.Add64(0, D, C)

	for j := 1; j < 34; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		t[34], D = bits.Add64(t[34], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		t[33], C = bits.Add64(t[34], C, 0)
		t[34], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)

	if D != 0 && t[34] == 0 {
		// reduction was not necessary
		copy(z[:], t[:34])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled2240(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[35]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[35]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[35]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[35]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [36]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)

	t[35], D = bits.Add64(t[35], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	t[34], C = bits.Add64(t[35], C, 0)
	t[35], _ = bits.Add64(0, D, C)

	for j := 1; j < 35; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		t[35], D = bits.Add64(t[35], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		t[34], C = bits.Add64(t[35], C, 0)
		t[35], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)

	if D != 0 && t[35] == 0 {
		// reduction was not necessary
		copy(z[:], t[:35])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled2304(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[36]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[36]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[36]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[36]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [37]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)

	t[36], D = bits.Add64(t[36], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	t[35], C = bits.Add64(t[36], C, 0)
	t[36], _ = bits.Add64(0, D, C)

	for j := 1; j < 36; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		t[36], D = bits.Add64(t[36], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		t[35], C = bits.Add64(t[36], C, 0)
		t[36], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)

	if D != 0 && t[36] == 0 {
		// reduction was not necessary
		copy(z[:], t[:36])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled2368(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[37]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[37]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[37]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[37]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [38]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)

	t[37], D = bits.Add64(t[37], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	t[36], C = bits.Add64(t[37], C, 0)
	t[37], _ = bits.Add64(0, D, C)

	for j := 1; j < 37; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		t[37], D = bits.Add64(t[37], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		t[36], C = bits.Add64(t[37], C, 0)
		t[37], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)

	if D != 0 && t[37] == 0 {
		// reduction was not necessary
		copy(z[:], t[:37])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled2432(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[38]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[38]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[38]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[38]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [39]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)

	t[38], D = bits.Add64(t[38], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	t[37], C = bits.Add64(t[38], C, 0)
	t[38], _ = bits.Add64(0, D, C)

	for j := 1; j < 38; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		t[38], D = bits.Add64(t[38], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		t[37], C = bits.Add64(t[38], C, 0)
		t[38], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)

	if D != 0 && t[38] == 0 {
		// reduction was not necessary
		copy(z[:], t[:38])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled2496(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[39]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[39]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[39]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[39]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [40]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)

	t[39], D = bits.Add64(t[39], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	t[38], C = bits.Add64(t[39], C, 0)
	t[39], _ = bits.Add64(0, D, C)

	for j := 1; j < 39; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		t[39], D = bits.Add64(t[39], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		t[38], C = bits.Add64(t[39], C, 0)
		t[39], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)

	if D != 0 && t[39] == 0 {
		// reduction was not necessary
		copy(z[:], t[:39])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled2560(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[40]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[40]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[40]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[40]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [41]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)

	t[40], D = bits.Add64(t[40], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	t[39], C = bits.Add64(t[40], C, 0)
	t[40], _ = bits.Add64(0, D, C)

	for j := 1; j < 40; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		t[40], D = bits.Add64(t[40], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		t[39], C = bits.Add64(t[40], C, 0)
		t[40], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)

	if D != 0 && t[40] == 0 {
		// reduction was not necessary
		copy(z[:], t[:40])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled2624(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[41]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[41]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[41]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[41]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [42]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)

	t[41], D = bits.Add64(t[41], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	t[40], C = bits.Add64(t[41], C, 0)
	t[41], _ = bits.Add64(0, D, C)

	for j := 1; j < 41; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		t[41], D = bits.Add64(t[41], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		t[40], C = bits.Add64(t[41], C, 0)
		t[41], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)

	if D != 0 && t[41] == 0 {
		// reduction was not necessary
		copy(z[:], t[:41])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled2688(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[42]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[42]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[42]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[42]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [43]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)

	t[42], D = bits.Add64(t[42], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	t[41], C = bits.Add64(t[42], C, 0)
	t[42], _ = bits.Add64(0, D, C)

	for j := 1; j < 42; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		t[42], D = bits.Add64(t[42], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		t[41], C = bits.Add64(t[42], C, 0)
		t[42], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)

	if D != 0 && t[42] == 0 {
		// reduction was not necessary
		copy(z[:], t[:42])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled2752(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[43]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[43]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[43]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[43]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [44]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC1 = bits.Sub64(mod[42], x[42], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)
	_, gteC2 = bits.Sub64(mod[42], y[42], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)
	C, t[42] = madd1(x[0], y[42], C)

	t[43], D = bits.Add64(t[43], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	C, t[41] = madd2(m, mod[42], t[42], C)
	t[42], C = bits.Add64(t[43], C, 0)
	t[43], _ = bits.Add64(0, D, C)

	for j := 1; j < 43; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		C, t[42] = madd2(x[j], y[42], t[42], C)
		t[43], D = bits.Add64(t[43], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		C, t[41] = madd2(m, mod[42], t[42], C)
		t[42], C = bits.Add64(t[43], C, 0)
		t[43], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)
	z[42], D = bits.Sub64(t[42], mod[42], D)

	if D != 0 && t[43] == 0 {
		// reduction was not necessary
		copy(z[:], t[:43])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled2816(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[44]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[44]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[44]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[44]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [45]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC1 = bits.Sub64(mod[42], x[42], gteC1)
	_, gteC1 = bits.Sub64(mod[43], x[43], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)
	_, gteC2 = bits.Sub64(mod[42], y[42], gteC2)
	_, gteC2 = bits.Sub64(mod[43], y[43], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)
	C, t[42] = madd1(x[0], y[42], C)
	C, t[43] = madd1(x[0], y[43], C)

	t[44], D = bits.Add64(t[44], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	C, t[41] = madd2(m, mod[42], t[42], C)
	C, t[42] = madd2(m, mod[43], t[43], C)
	t[43], C = bits.Add64(t[44], C, 0)
	t[44], _ = bits.Add64(0, D, C)

	for j := 1; j < 44; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		C, t[42] = madd2(x[j], y[42], t[42], C)
		C, t[43] = madd2(x[j], y[43], t[43], C)
		t[44], D = bits.Add64(t[44], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		C, t[41] = madd2(m, mod[42], t[42], C)
		C, t[42] = madd2(m, mod[43], t[43], C)
		t[43], C = bits.Add64(t[44], C, 0)
		t[44], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)
	z[42], D = bits.Sub64(t[42], mod[42], D)
	z[43], D = bits.Sub64(t[43], mod[43], D)

	if D != 0 && t[44] == 0 {
		// reduction was not necessary
		copy(z[:], t[:44])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled2880(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[45]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[45]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[45]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[45]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [46]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC1 = bits.Sub64(mod[42], x[42], gteC1)
	_, gteC1 = bits.Sub64(mod[43], x[43], gteC1)
	_, gteC1 = bits.Sub64(mod[44], x[44], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)
	_, gteC2 = bits.Sub64(mod[42], y[42], gteC2)
	_, gteC2 = bits.Sub64(mod[43], y[43], gteC2)
	_, gteC2 = bits.Sub64(mod[44], y[44], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)
	C, t[42] = madd1(x[0], y[42], C)
	C, t[43] = madd1(x[0], y[43], C)
	C, t[44] = madd1(x[0], y[44], C)

	t[45], D = bits.Add64(t[45], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	C, t[41] = madd2(m, mod[42], t[42], C)
	C, t[42] = madd2(m, mod[43], t[43], C)
	C, t[43] = madd2(m, mod[44], t[44], C)
	t[44], C = bits.Add64(t[45], C, 0)
	t[45], _ = bits.Add64(0, D, C)

	for j := 1; j < 45; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		C, t[42] = madd2(x[j], y[42], t[42], C)
		C, t[43] = madd2(x[j], y[43], t[43], C)
		C, t[44] = madd2(x[j], y[44], t[44], C)
		t[45], D = bits.Add64(t[45], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		C, t[41] = madd2(m, mod[42], t[42], C)
		C, t[42] = madd2(m, mod[43], t[43], C)
		C, t[43] = madd2(m, mod[44], t[44], C)
		t[44], C = bits.Add64(t[45], C, 0)
		t[45], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)
	z[42], D = bits.Sub64(t[42], mod[42], D)
	z[43], D = bits.Sub64(t[43], mod[43], D)
	z[44], D = bits.Sub64(t[44], mod[44], D)

	if D != 0 && t[45] == 0 {
		// reduction was not necessary
		copy(z[:], t[:45])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled2944(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[46]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[46]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[46]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[46]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [47]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC1 = bits.Sub64(mod[42], x[42], gteC1)
	_, gteC1 = bits.Sub64(mod[43], x[43], gteC1)
	_, gteC1 = bits.Sub64(mod[44], x[44], gteC1)
	_, gteC1 = bits.Sub64(mod[45], x[45], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)
	_, gteC2 = bits.Sub64(mod[42], y[42], gteC2)
	_, gteC2 = bits.Sub64(mod[43], y[43], gteC2)
	_, gteC2 = bits.Sub64(mod[44], y[44], gteC2)
	_, gteC2 = bits.Sub64(mod[45], y[45], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)
	C, t[42] = madd1(x[0], y[42], C)
	C, t[43] = madd1(x[0], y[43], C)
	C, t[44] = madd1(x[0], y[44], C)
	C, t[45] = madd1(x[0], y[45], C)

	t[46], D = bits.Add64(t[46], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	C, t[41] = madd2(m, mod[42], t[42], C)
	C, t[42] = madd2(m, mod[43], t[43], C)
	C, t[43] = madd2(m, mod[44], t[44], C)
	C, t[44] = madd2(m, mod[45], t[45], C)
	t[45], C = bits.Add64(t[46], C, 0)
	t[46], _ = bits.Add64(0, D, C)

	for j := 1; j < 46; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		C, t[42] = madd2(x[j], y[42], t[42], C)
		C, t[43] = madd2(x[j], y[43], t[43], C)
		C, t[44] = madd2(x[j], y[44], t[44], C)
		C, t[45] = madd2(x[j], y[45], t[45], C)
		t[46], D = bits.Add64(t[46], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		C, t[41] = madd2(m, mod[42], t[42], C)
		C, t[42] = madd2(m, mod[43], t[43], C)
		C, t[43] = madd2(m, mod[44], t[44], C)
		C, t[44] = madd2(m, mod[45], t[45], C)
		t[45], C = bits.Add64(t[46], C, 0)
		t[46], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)
	z[42], D = bits.Sub64(t[42], mod[42], D)
	z[43], D = bits.Sub64(t[43], mod[43], D)
	z[44], D = bits.Sub64(t[44], mod[44], D)
	z[45], D = bits.Sub64(t[45], mod[45], D)

	if D != 0 && t[46] == 0 {
		// reduction was not necessary
		copy(z[:], t[:46])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled3008(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[47]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[47]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[47]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[47]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [48]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC1 = bits.Sub64(mod[42], x[42], gteC1)
	_, gteC1 = bits.Sub64(mod[43], x[43], gteC1)
	_, gteC1 = bits.Sub64(mod[44], x[44], gteC1)
	_, gteC1 = bits.Sub64(mod[45], x[45], gteC1)
	_, gteC1 = bits.Sub64(mod[46], x[46], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)
	_, gteC2 = bits.Sub64(mod[42], y[42], gteC2)
	_, gteC2 = bits.Sub64(mod[43], y[43], gteC2)
	_, gteC2 = bits.Sub64(mod[44], y[44], gteC2)
	_, gteC2 = bits.Sub64(mod[45], y[45], gteC2)
	_, gteC2 = bits.Sub64(mod[46], y[46], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)
	C, t[42] = madd1(x[0], y[42], C)
	C, t[43] = madd1(x[0], y[43], C)
	C, t[44] = madd1(x[0], y[44], C)
	C, t[45] = madd1(x[0], y[45], C)
	C, t[46] = madd1(x[0], y[46], C)

	t[47], D = bits.Add64(t[47], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	C, t[41] = madd2(m, mod[42], t[42], C)
	C, t[42] = madd2(m, mod[43], t[43], C)
	C, t[43] = madd2(m, mod[44], t[44], C)
	C, t[44] = madd2(m, mod[45], t[45], C)
	C, t[45] = madd2(m, mod[46], t[46], C)
	t[46], C = bits.Add64(t[47], C, 0)
	t[47], _ = bits.Add64(0, D, C)

	for j := 1; j < 47; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		C, t[42] = madd2(x[j], y[42], t[42], C)
		C, t[43] = madd2(x[j], y[43], t[43], C)
		C, t[44] = madd2(x[j], y[44], t[44], C)
		C, t[45] = madd2(x[j], y[45], t[45], C)
		C, t[46] = madd2(x[j], y[46], t[46], C)
		t[47], D = bits.Add64(t[47], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		C, t[41] = madd2(m, mod[42], t[42], C)
		C, t[42] = madd2(m, mod[43], t[43], C)
		C, t[43] = madd2(m, mod[44], t[44], C)
		C, t[44] = madd2(m, mod[45], t[45], C)
		C, t[45] = madd2(m, mod[46], t[46], C)
		t[46], C = bits.Add64(t[47], C, 0)
		t[47], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)
	z[42], D = bits.Sub64(t[42], mod[42], D)
	z[43], D = bits.Sub64(t[43], mod[43], D)
	z[44], D = bits.Sub64(t[44], mod[44], D)
	z[45], D = bits.Sub64(t[45], mod[45], D)
	z[46], D = bits.Sub64(t[46], mod[46], D)

	if D != 0 && t[47] == 0 {
		// reduction was not necessary
		copy(z[:], t[:47])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled3072(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[48]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[48]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[48]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[48]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [49]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC1 = bits.Sub64(mod[42], x[42], gteC1)
	_, gteC1 = bits.Sub64(mod[43], x[43], gteC1)
	_, gteC1 = bits.Sub64(mod[44], x[44], gteC1)
	_, gteC1 = bits.Sub64(mod[45], x[45], gteC1)
	_, gteC1 = bits.Sub64(mod[46], x[46], gteC1)
	_, gteC1 = bits.Sub64(mod[47], x[47], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)
	_, gteC2 = bits.Sub64(mod[42], y[42], gteC2)
	_, gteC2 = bits.Sub64(mod[43], y[43], gteC2)
	_, gteC2 = bits.Sub64(mod[44], y[44], gteC2)
	_, gteC2 = bits.Sub64(mod[45], y[45], gteC2)
	_, gteC2 = bits.Sub64(mod[46], y[46], gteC2)
	_, gteC2 = bits.Sub64(mod[47], y[47], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)
	C, t[42] = madd1(x[0], y[42], C)
	C, t[43] = madd1(x[0], y[43], C)
	C, t[44] = madd1(x[0], y[44], C)
	C, t[45] = madd1(x[0], y[45], C)
	C, t[46] = madd1(x[0], y[46], C)
	C, t[47] = madd1(x[0], y[47], C)

	t[48], D = bits.Add64(t[48], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	C, t[41] = madd2(m, mod[42], t[42], C)
	C, t[42] = madd2(m, mod[43], t[43], C)
	C, t[43] = madd2(m, mod[44], t[44], C)
	C, t[44] = madd2(m, mod[45], t[45], C)
	C, t[45] = madd2(m, mod[46], t[46], C)
	C, t[46] = madd2(m, mod[47], t[47], C)
	t[47], C = bits.Add64(t[48], C, 0)
	t[48], _ = bits.Add64(0, D, C)

	for j := 1; j < 48; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		C, t[42] = madd2(x[j], y[42], t[42], C)
		C, t[43] = madd2(x[j], y[43], t[43], C)
		C, t[44] = madd2(x[j], y[44], t[44], C)
		C, t[45] = madd2(x[j], y[45], t[45], C)
		C, t[46] = madd2(x[j], y[46], t[46], C)
		C, t[47] = madd2(x[j], y[47], t[47], C)
		t[48], D = bits.Add64(t[48], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		C, t[41] = madd2(m, mod[42], t[42], C)
		C, t[42] = madd2(m, mod[43], t[43], C)
		C, t[43] = madd2(m, mod[44], t[44], C)
		C, t[44] = madd2(m, mod[45], t[45], C)
		C, t[45] = madd2(m, mod[46], t[46], C)
		C, t[46] = madd2(m, mod[47], t[47], C)
		t[47], C = bits.Add64(t[48], C, 0)
		t[48], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)
	z[42], D = bits.Sub64(t[42], mod[42], D)
	z[43], D = bits.Sub64(t[43], mod[43], D)
	z[44], D = bits.Sub64(t[44], mod[44], D)
	z[45], D = bits.Sub64(t[45], mod[45], D)
	z[46], D = bits.Sub64(t[46], mod[46], D)
	z[47], D = bits.Sub64(t[47], mod[47], D)

	if D != 0 && t[48] == 0 {
		// reduction was not necessary
		copy(z[:], t[:48])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled3136(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[49]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[49]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[49]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[49]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [50]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC1 = bits.Sub64(mod[42], x[42], gteC1)
	_, gteC1 = bits.Sub64(mod[43], x[43], gteC1)
	_, gteC1 = bits.Sub64(mod[44], x[44], gteC1)
	_, gteC1 = bits.Sub64(mod[45], x[45], gteC1)
	_, gteC1 = bits.Sub64(mod[46], x[46], gteC1)
	_, gteC1 = bits.Sub64(mod[47], x[47], gteC1)
	_, gteC1 = bits.Sub64(mod[48], x[48], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)
	_, gteC2 = bits.Sub64(mod[42], y[42], gteC2)
	_, gteC2 = bits.Sub64(mod[43], y[43], gteC2)
	_, gteC2 = bits.Sub64(mod[44], y[44], gteC2)
	_, gteC2 = bits.Sub64(mod[45], y[45], gteC2)
	_, gteC2 = bits.Sub64(mod[46], y[46], gteC2)
	_, gteC2 = bits.Sub64(mod[47], y[47], gteC2)
	_, gteC2 = bits.Sub64(mod[48], y[48], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)
	C, t[42] = madd1(x[0], y[42], C)
	C, t[43] = madd1(x[0], y[43], C)
	C, t[44] = madd1(x[0], y[44], C)
	C, t[45] = madd1(x[0], y[45], C)
	C, t[46] = madd1(x[0], y[46], C)
	C, t[47] = madd1(x[0], y[47], C)
	C, t[48] = madd1(x[0], y[48], C)

	t[49], D = bits.Add64(t[49], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	C, t[41] = madd2(m, mod[42], t[42], C)
	C, t[42] = madd2(m, mod[43], t[43], C)
	C, t[43] = madd2(m, mod[44], t[44], C)
	C, t[44] = madd2(m, mod[45], t[45], C)
	C, t[45] = madd2(m, mod[46], t[46], C)
	C, t[46] = madd2(m, mod[47], t[47], C)
	C, t[47] = madd2(m, mod[48], t[48], C)
	t[48], C = bits.Add64(t[49], C, 0)
	t[49], _ = bits.Add64(0, D, C)

	for j := 1; j < 49; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		C, t[42] = madd2(x[j], y[42], t[42], C)
		C, t[43] = madd2(x[j], y[43], t[43], C)
		C, t[44] = madd2(x[j], y[44], t[44], C)
		C, t[45] = madd2(x[j], y[45], t[45], C)
		C, t[46] = madd2(x[j], y[46], t[46], C)
		C, t[47] = madd2(x[j], y[47], t[47], C)
		C, t[48] = madd2(x[j], y[48], t[48], C)
		t[49], D = bits.Add64(t[49], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		C, t[41] = madd2(m, mod[42], t[42], C)
		C, t[42] = madd2(m, mod[43], t[43], C)
		C, t[43] = madd2(m, mod[44], t[44], C)
		C, t[44] = madd2(m, mod[45], t[45], C)
		C, t[45] = madd2(m, mod[46], t[46], C)
		C, t[46] = madd2(m, mod[47], t[47], C)
		C, t[47] = madd2(m, mod[48], t[48], C)
		t[48], C = bits.Add64(t[49], C, 0)
		t[49], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)
	z[42], D = bits.Sub64(t[42], mod[42], D)
	z[43], D = bits.Sub64(t[43], mod[43], D)
	z[44], D = bits.Sub64(t[44], mod[44], D)
	z[45], D = bits.Sub64(t[45], mod[45], D)
	z[46], D = bits.Sub64(t[46], mod[46], D)
	z[47], D = bits.Sub64(t[47], mod[47], D)
	z[48], D = bits.Sub64(t[48], mod[48], D)

	if D != 0 && t[49] == 0 {
		// reduction was not necessary
		copy(z[:], t[:49])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled3200(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[50]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[50]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[50]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[50]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [51]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC1 = bits.Sub64(mod[42], x[42], gteC1)
	_, gteC1 = bits.Sub64(mod[43], x[43], gteC1)
	_, gteC1 = bits.Sub64(mod[44], x[44], gteC1)
	_, gteC1 = bits.Sub64(mod[45], x[45], gteC1)
	_, gteC1 = bits.Sub64(mod[46], x[46], gteC1)
	_, gteC1 = bits.Sub64(mod[47], x[47], gteC1)
	_, gteC1 = bits.Sub64(mod[48], x[48], gteC1)
	_, gteC1 = bits.Sub64(mod[49], x[49], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)
	_, gteC2 = bits.Sub64(mod[42], y[42], gteC2)
	_, gteC2 = bits.Sub64(mod[43], y[43], gteC2)
	_, gteC2 = bits.Sub64(mod[44], y[44], gteC2)
	_, gteC2 = bits.Sub64(mod[45], y[45], gteC2)
	_, gteC2 = bits.Sub64(mod[46], y[46], gteC2)
	_, gteC2 = bits.Sub64(mod[47], y[47], gteC2)
	_, gteC2 = bits.Sub64(mod[48], y[48], gteC2)
	_, gteC2 = bits.Sub64(mod[49], y[49], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)
	C, t[42] = madd1(x[0], y[42], C)
	C, t[43] = madd1(x[0], y[43], C)
	C, t[44] = madd1(x[0], y[44], C)
	C, t[45] = madd1(x[0], y[45], C)
	C, t[46] = madd1(x[0], y[46], C)
	C, t[47] = madd1(x[0], y[47], C)
	C, t[48] = madd1(x[0], y[48], C)
	C, t[49] = madd1(x[0], y[49], C)

	t[50], D = bits.Add64(t[50], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	C, t[41] = madd2(m, mod[42], t[42], C)
	C, t[42] = madd2(m, mod[43], t[43], C)
	C, t[43] = madd2(m, mod[44], t[44], C)
	C, t[44] = madd2(m, mod[45], t[45], C)
	C, t[45] = madd2(m, mod[46], t[46], C)
	C, t[46] = madd2(m, mod[47], t[47], C)
	C, t[47] = madd2(m, mod[48], t[48], C)
	C, t[48] = madd2(m, mod[49], t[49], C)
	t[49], C = bits.Add64(t[50], C, 0)
	t[50], _ = bits.Add64(0, D, C)

	for j := 1; j < 50; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		C, t[42] = madd2(x[j], y[42], t[42], C)
		C, t[43] = madd2(x[j], y[43], t[43], C)
		C, t[44] = madd2(x[j], y[44], t[44], C)
		C, t[45] = madd2(x[j], y[45], t[45], C)
		C, t[46] = madd2(x[j], y[46], t[46], C)
		C, t[47] = madd2(x[j], y[47], t[47], C)
		C, t[48] = madd2(x[j], y[48], t[48], C)
		C, t[49] = madd2(x[j], y[49], t[49], C)
		t[50], D = bits.Add64(t[50], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		C, t[41] = madd2(m, mod[42], t[42], C)
		C, t[42] = madd2(m, mod[43], t[43], C)
		C, t[43] = madd2(m, mod[44], t[44], C)
		C, t[44] = madd2(m, mod[45], t[45], C)
		C, t[45] = madd2(m, mod[46], t[46], C)
		C, t[46] = madd2(m, mod[47], t[47], C)
		C, t[47] = madd2(m, mod[48], t[48], C)
		C, t[48] = madd2(m, mod[49], t[49], C)
		t[49], C = bits.Add64(t[50], C, 0)
		t[50], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)
	z[42], D = bits.Sub64(t[42], mod[42], D)
	z[43], D = bits.Sub64(t[43], mod[43], D)
	z[44], D = bits.Sub64(t[44], mod[44], D)
	z[45], D = bits.Sub64(t[45], mod[45], D)
	z[46], D = bits.Sub64(t[46], mod[46], D)
	z[47], D = bits.Sub64(t[47], mod[47], D)
	z[48], D = bits.Sub64(t[48], mod[48], D)
	z[49], D = bits.Sub64(t[49], mod[49], D)

	if D != 0 && t[50] == 0 {
		// reduction was not necessary
		copy(z[:], t[:50])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled3264(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[51]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[51]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[51]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[51]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [52]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC1 = bits.Sub64(mod[42], x[42], gteC1)
	_, gteC1 = bits.Sub64(mod[43], x[43], gteC1)
	_, gteC1 = bits.Sub64(mod[44], x[44], gteC1)
	_, gteC1 = bits.Sub64(mod[45], x[45], gteC1)
	_, gteC1 = bits.Sub64(mod[46], x[46], gteC1)
	_, gteC1 = bits.Sub64(mod[47], x[47], gteC1)
	_, gteC1 = bits.Sub64(mod[48], x[48], gteC1)
	_, gteC1 = bits.Sub64(mod[49], x[49], gteC1)
	_, gteC1 = bits.Sub64(mod[50], x[50], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)
	_, gteC2 = bits.Sub64(mod[42], y[42], gteC2)
	_, gteC2 = bits.Sub64(mod[43], y[43], gteC2)
	_, gteC2 = bits.Sub64(mod[44], y[44], gteC2)
	_, gteC2 = bits.Sub64(mod[45], y[45], gteC2)
	_, gteC2 = bits.Sub64(mod[46], y[46], gteC2)
	_, gteC2 = bits.Sub64(mod[47], y[47], gteC2)
	_, gteC2 = bits.Sub64(mod[48], y[48], gteC2)
	_, gteC2 = bits.Sub64(mod[49], y[49], gteC2)
	_, gteC2 = bits.Sub64(mod[50], y[50], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)
	C, t[42] = madd1(x[0], y[42], C)
	C, t[43] = madd1(x[0], y[43], C)
	C, t[44] = madd1(x[0], y[44], C)
	C, t[45] = madd1(x[0], y[45], C)
	C, t[46] = madd1(x[0], y[46], C)
	C, t[47] = madd1(x[0], y[47], C)
	C, t[48] = madd1(x[0], y[48], C)
	C, t[49] = madd1(x[0], y[49], C)
	C, t[50] = madd1(x[0], y[50], C)

	t[51], D = bits.Add64(t[51], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	C, t[41] = madd2(m, mod[42], t[42], C)
	C, t[42] = madd2(m, mod[43], t[43], C)
	C, t[43] = madd2(m, mod[44], t[44], C)
	C, t[44] = madd2(m, mod[45], t[45], C)
	C, t[45] = madd2(m, mod[46], t[46], C)
	C, t[46] = madd2(m, mod[47], t[47], C)
	C, t[47] = madd2(m, mod[48], t[48], C)
	C, t[48] = madd2(m, mod[49], t[49], C)
	C, t[49] = madd2(m, mod[50], t[50], C)
	t[50], C = bits.Add64(t[51], C, 0)
	t[51], _ = bits.Add64(0, D, C)

	for j := 1; j < 51; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		C, t[42] = madd2(x[j], y[42], t[42], C)
		C, t[43] = madd2(x[j], y[43], t[43], C)
		C, t[44] = madd2(x[j], y[44], t[44], C)
		C, t[45] = madd2(x[j], y[45], t[45], C)
		C, t[46] = madd2(x[j], y[46], t[46], C)
		C, t[47] = madd2(x[j], y[47], t[47], C)
		C, t[48] = madd2(x[j], y[48], t[48], C)
		C, t[49] = madd2(x[j], y[49], t[49], C)
		C, t[50] = madd2(x[j], y[50], t[50], C)
		t[51], D = bits.Add64(t[51], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		C, t[41] = madd2(m, mod[42], t[42], C)
		C, t[42] = madd2(m, mod[43], t[43], C)
		C, t[43] = madd2(m, mod[44], t[44], C)
		C, t[44] = madd2(m, mod[45], t[45], C)
		C, t[45] = madd2(m, mod[46], t[46], C)
		C, t[46] = madd2(m, mod[47], t[47], C)
		C, t[47] = madd2(m, mod[48], t[48], C)
		C, t[48] = madd2(m, mod[49], t[49], C)
		C, t[49] = madd2(m, mod[50], t[50], C)
		t[50], C = bits.Add64(t[51], C, 0)
		t[51], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)
	z[42], D = bits.Sub64(t[42], mod[42], D)
	z[43], D = bits.Sub64(t[43], mod[43], D)
	z[44], D = bits.Sub64(t[44], mod[44], D)
	z[45], D = bits.Sub64(t[45], mod[45], D)
	z[46], D = bits.Sub64(t[46], mod[46], D)
	z[47], D = bits.Sub64(t[47], mod[47], D)
	z[48], D = bits.Sub64(t[48], mod[48], D)
	z[49], D = bits.Sub64(t[49], mod[49], D)
	z[50], D = bits.Sub64(t[50], mod[50], D)

	if D != 0 && t[51] == 0 {
		// reduction was not necessary
		copy(z[:], t[:51])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled3328(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[52]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[52]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[52]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[52]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [53]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC1 = bits.Sub64(mod[42], x[42], gteC1)
	_, gteC1 = bits.Sub64(mod[43], x[43], gteC1)
	_, gteC1 = bits.Sub64(mod[44], x[44], gteC1)
	_, gteC1 = bits.Sub64(mod[45], x[45], gteC1)
	_, gteC1 = bits.Sub64(mod[46], x[46], gteC1)
	_, gteC1 = bits.Sub64(mod[47], x[47], gteC1)
	_, gteC1 = bits.Sub64(mod[48], x[48], gteC1)
	_, gteC1 = bits.Sub64(mod[49], x[49], gteC1)
	_, gteC1 = bits.Sub64(mod[50], x[50], gteC1)
	_, gteC1 = bits.Sub64(mod[51], x[51], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)
	_, gteC2 = bits.Sub64(mod[42], y[42], gteC2)
	_, gteC2 = bits.Sub64(mod[43], y[43], gteC2)
	_, gteC2 = bits.Sub64(mod[44], y[44], gteC2)
	_, gteC2 = bits.Sub64(mod[45], y[45], gteC2)
	_, gteC2 = bits.Sub64(mod[46], y[46], gteC2)
	_, gteC2 = bits.Sub64(mod[47], y[47], gteC2)
	_, gteC2 = bits.Sub64(mod[48], y[48], gteC2)
	_, gteC2 = bits.Sub64(mod[49], y[49], gteC2)
	_, gteC2 = bits.Sub64(mod[50], y[50], gteC2)
	_, gteC2 = bits.Sub64(mod[51], y[51], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)
	C, t[42] = madd1(x[0], y[42], C)
	C, t[43] = madd1(x[0], y[43], C)
	C, t[44] = madd1(x[0], y[44], C)
	C, t[45] = madd1(x[0], y[45], C)
	C, t[46] = madd1(x[0], y[46], C)
	C, t[47] = madd1(x[0], y[47], C)
	C, t[48] = madd1(x[0], y[48], C)
	C, t[49] = madd1(x[0], y[49], C)
	C, t[50] = madd1(x[0], y[50], C)
	C, t[51] = madd1(x[0], y[51], C)

	t[52], D = bits.Add64(t[52], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	C, t[41] = madd2(m, mod[42], t[42], C)
	C, t[42] = madd2(m, mod[43], t[43], C)
	C, t[43] = madd2(m, mod[44], t[44], C)
	C, t[44] = madd2(m, mod[45], t[45], C)
	C, t[45] = madd2(m, mod[46], t[46], C)
	C, t[46] = madd2(m, mod[47], t[47], C)
	C, t[47] = madd2(m, mod[48], t[48], C)
	C, t[48] = madd2(m, mod[49], t[49], C)
	C, t[49] = madd2(m, mod[50], t[50], C)
	C, t[50] = madd2(m, mod[51], t[51], C)
	t[51], C = bits.Add64(t[52], C, 0)
	t[52], _ = bits.Add64(0, D, C)

	for j := 1; j < 52; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		C, t[42] = madd2(x[j], y[42], t[42], C)
		C, t[43] = madd2(x[j], y[43], t[43], C)
		C, t[44] = madd2(x[j], y[44], t[44], C)
		C, t[45] = madd2(x[j], y[45], t[45], C)
		C, t[46] = madd2(x[j], y[46], t[46], C)
		C, t[47] = madd2(x[j], y[47], t[47], C)
		C, t[48] = madd2(x[j], y[48], t[48], C)
		C, t[49] = madd2(x[j], y[49], t[49], C)
		C, t[50] = madd2(x[j], y[50], t[50], C)
		C, t[51] = madd2(x[j], y[51], t[51], C)
		t[52], D = bits.Add64(t[52], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		C, t[41] = madd2(m, mod[42], t[42], C)
		C, t[42] = madd2(m, mod[43], t[43], C)
		C, t[43] = madd2(m, mod[44], t[44], C)
		C, t[44] = madd2(m, mod[45], t[45], C)
		C, t[45] = madd2(m, mod[46], t[46], C)
		C, t[46] = madd2(m, mod[47], t[47], C)
		C, t[47] = madd2(m, mod[48], t[48], C)
		C, t[48] = madd2(m, mod[49], t[49], C)
		C, t[49] = madd2(m, mod[50], t[50], C)
		C, t[50] = madd2(m, mod[51], t[51], C)
		t[51], C = bits.Add64(t[52], C, 0)
		t[52], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)
	z[42], D = bits.Sub64(t[42], mod[42], D)
	z[43], D = bits.Sub64(t[43], mod[43], D)
	z[44], D = bits.Sub64(t[44], mod[44], D)
	z[45], D = bits.Sub64(t[45], mod[45], D)
	z[46], D = bits.Sub64(t[46], mod[46], D)
	z[47], D = bits.Sub64(t[47], mod[47], D)
	z[48], D = bits.Sub64(t[48], mod[48], D)
	z[49], D = bits.Sub64(t[49], mod[49], D)
	z[50], D = bits.Sub64(t[50], mod[50], D)
	z[51], D = bits.Sub64(t[51], mod[51], D)

	if D != 0 && t[52] == 0 {
		// reduction was not necessary
		copy(z[:], t[:52])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled3392(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[53]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[53]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[53]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[53]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [54]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC1 = bits.Sub64(mod[42], x[42], gteC1)
	_, gteC1 = bits.Sub64(mod[43], x[43], gteC1)
	_, gteC1 = bits.Sub64(mod[44], x[44], gteC1)
	_, gteC1 = bits.Sub64(mod[45], x[45], gteC1)
	_, gteC1 = bits.Sub64(mod[46], x[46], gteC1)
	_, gteC1 = bits.Sub64(mod[47], x[47], gteC1)
	_, gteC1 = bits.Sub64(mod[48], x[48], gteC1)
	_, gteC1 = bits.Sub64(mod[49], x[49], gteC1)
	_, gteC1 = bits.Sub64(mod[50], x[50], gteC1)
	_, gteC1 = bits.Sub64(mod[51], x[51], gteC1)
	_, gteC1 = bits.Sub64(mod[52], x[52], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)
	_, gteC2 = bits.Sub64(mod[42], y[42], gteC2)
	_, gteC2 = bits.Sub64(mod[43], y[43], gteC2)
	_, gteC2 = bits.Sub64(mod[44], y[44], gteC2)
	_, gteC2 = bits.Sub64(mod[45], y[45], gteC2)
	_, gteC2 = bits.Sub64(mod[46], y[46], gteC2)
	_, gteC2 = bits.Sub64(mod[47], y[47], gteC2)
	_, gteC2 = bits.Sub64(mod[48], y[48], gteC2)
	_, gteC2 = bits.Sub64(mod[49], y[49], gteC2)
	_, gteC2 = bits.Sub64(mod[50], y[50], gteC2)
	_, gteC2 = bits.Sub64(mod[51], y[51], gteC2)
	_, gteC2 = bits.Sub64(mod[52], y[52], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)
	C, t[42] = madd1(x[0], y[42], C)
	C, t[43] = madd1(x[0], y[43], C)
	C, t[44] = madd1(x[0], y[44], C)
	C, t[45] = madd1(x[0], y[45], C)
	C, t[46] = madd1(x[0], y[46], C)
	C, t[47] = madd1(x[0], y[47], C)
	C, t[48] = madd1(x[0], y[48], C)
	C, t[49] = madd1(x[0], y[49], C)
	C, t[50] = madd1(x[0], y[50], C)
	C, t[51] = madd1(x[0], y[51], C)
	C, t[52] = madd1(x[0], y[52], C)

	t[53], D = bits.Add64(t[53], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	C, t[41] = madd2(m, mod[42], t[42], C)
	C, t[42] = madd2(m, mod[43], t[43], C)
	C, t[43] = madd2(m, mod[44], t[44], C)
	C, t[44] = madd2(m, mod[45], t[45], C)
	C, t[45] = madd2(m, mod[46], t[46], C)
	C, t[46] = madd2(m, mod[47], t[47], C)
	C, t[47] = madd2(m, mod[48], t[48], C)
	C, t[48] = madd2(m, mod[49], t[49], C)
	C, t[49] = madd2(m, mod[50], t[50], C)
	C, t[50] = madd2(m, mod[51], t[51], C)
	C, t[51] = madd2(m, mod[52], t[52], C)
	t[52], C = bits.Add64(t[53], C, 0)
	t[53], _ = bits.Add64(0, D, C)

	for j := 1; j < 53; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		C, t[42] = madd2(x[j], y[42], t[42], C)
		C, t[43] = madd2(x[j], y[43], t[43], C)
		C, t[44] = madd2(x[j], y[44], t[44], C)
		C, t[45] = madd2(x[j], y[45], t[45], C)
		C, t[46] = madd2(x[j], y[46], t[46], C)
		C, t[47] = madd2(x[j], y[47], t[47], C)
		C, t[48] = madd2(x[j], y[48], t[48], C)
		C, t[49] = madd2(x[j], y[49], t[49], C)
		C, t[50] = madd2(x[j], y[50], t[50], C)
		C, t[51] = madd2(x[j], y[51], t[51], C)
		C, t[52] = madd2(x[j], y[52], t[52], C)
		t[53], D = bits.Add64(t[53], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		C, t[41] = madd2(m, mod[42], t[42], C)
		C, t[42] = madd2(m, mod[43], t[43], C)
		C, t[43] = madd2(m, mod[44], t[44], C)
		C, t[44] = madd2(m, mod[45], t[45], C)
		C, t[45] = madd2(m, mod[46], t[46], C)
		C, t[46] = madd2(m, mod[47], t[47], C)
		C, t[47] = madd2(m, mod[48], t[48], C)
		C, t[48] = madd2(m, mod[49], t[49], C)
		C, t[49] = madd2(m, mod[50], t[50], C)
		C, t[50] = madd2(m, mod[51], t[51], C)
		C, t[51] = madd2(m, mod[52], t[52], C)
		t[52], C = bits.Add64(t[53], C, 0)
		t[53], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)
	z[42], D = bits.Sub64(t[42], mod[42], D)
	z[43], D = bits.Sub64(t[43], mod[43], D)
	z[44], D = bits.Sub64(t[44], mod[44], D)
	z[45], D = bits.Sub64(t[45], mod[45], D)
	z[46], D = bits.Sub64(t[46], mod[46], D)
	z[47], D = bits.Sub64(t[47], mod[47], D)
	z[48], D = bits.Sub64(t[48], mod[48], D)
	z[49], D = bits.Sub64(t[49], mod[49], D)
	z[50], D = bits.Sub64(t[50], mod[50], D)
	z[51], D = bits.Sub64(t[51], mod[51], D)
	z[52], D = bits.Sub64(t[52], mod[52], D)

	if D != 0 && t[53] == 0 {
		// reduction was not necessary
		copy(z[:], t[:53])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled3456(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[54]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[54]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[54]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[54]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [55]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC1 = bits.Sub64(mod[42], x[42], gteC1)
	_, gteC1 = bits.Sub64(mod[43], x[43], gteC1)
	_, gteC1 = bits.Sub64(mod[44], x[44], gteC1)
	_, gteC1 = bits.Sub64(mod[45], x[45], gteC1)
	_, gteC1 = bits.Sub64(mod[46], x[46], gteC1)
	_, gteC1 = bits.Sub64(mod[47], x[47], gteC1)
	_, gteC1 = bits.Sub64(mod[48], x[48], gteC1)
	_, gteC1 = bits.Sub64(mod[49], x[49], gteC1)
	_, gteC1 = bits.Sub64(mod[50], x[50], gteC1)
	_, gteC1 = bits.Sub64(mod[51], x[51], gteC1)
	_, gteC1 = bits.Sub64(mod[52], x[52], gteC1)
	_, gteC1 = bits.Sub64(mod[53], x[53], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)
	_, gteC2 = bits.Sub64(mod[42], y[42], gteC2)
	_, gteC2 = bits.Sub64(mod[43], y[43], gteC2)
	_, gteC2 = bits.Sub64(mod[44], y[44], gteC2)
	_, gteC2 = bits.Sub64(mod[45], y[45], gteC2)
	_, gteC2 = bits.Sub64(mod[46], y[46], gteC2)
	_, gteC2 = bits.Sub64(mod[47], y[47], gteC2)
	_, gteC2 = bits.Sub64(mod[48], y[48], gteC2)
	_, gteC2 = bits.Sub64(mod[49], y[49], gteC2)
	_, gteC2 = bits.Sub64(mod[50], y[50], gteC2)
	_, gteC2 = bits.Sub64(mod[51], y[51], gteC2)
	_, gteC2 = bits.Sub64(mod[52], y[52], gteC2)
	_, gteC2 = bits.Sub64(mod[53], y[53], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)
	C, t[42] = madd1(x[0], y[42], C)
	C, t[43] = madd1(x[0], y[43], C)
	C, t[44] = madd1(x[0], y[44], C)
	C, t[45] = madd1(x[0], y[45], C)
	C, t[46] = madd1(x[0], y[46], C)
	C, t[47] = madd1(x[0], y[47], C)
	C, t[48] = madd1(x[0], y[48], C)
	C, t[49] = madd1(x[0], y[49], C)
	C, t[50] = madd1(x[0], y[50], C)
	C, t[51] = madd1(x[0], y[51], C)
	C, t[52] = madd1(x[0], y[52], C)
	C, t[53] = madd1(x[0], y[53], C)

	t[54], D = bits.Add64(t[54], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	C, t[41] = madd2(m, mod[42], t[42], C)
	C, t[42] = madd2(m, mod[43], t[43], C)
	C, t[43] = madd2(m, mod[44], t[44], C)
	C, t[44] = madd2(m, mod[45], t[45], C)
	C, t[45] = madd2(m, mod[46], t[46], C)
	C, t[46] = madd2(m, mod[47], t[47], C)
	C, t[47] = madd2(m, mod[48], t[48], C)
	C, t[48] = madd2(m, mod[49], t[49], C)
	C, t[49] = madd2(m, mod[50], t[50], C)
	C, t[50] = madd2(m, mod[51], t[51], C)
	C, t[51] = madd2(m, mod[52], t[52], C)
	C, t[52] = madd2(m, mod[53], t[53], C)
	t[53], C = bits.Add64(t[54], C, 0)
	t[54], _ = bits.Add64(0, D, C)

	for j := 1; j < 54; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		C, t[42] = madd2(x[j], y[42], t[42], C)
		C, t[43] = madd2(x[j], y[43], t[43], C)
		C, t[44] = madd2(x[j], y[44], t[44], C)
		C, t[45] = madd2(x[j], y[45], t[45], C)
		C, t[46] = madd2(x[j], y[46], t[46], C)
		C, t[47] = madd2(x[j], y[47], t[47], C)
		C, t[48] = madd2(x[j], y[48], t[48], C)
		C, t[49] = madd2(x[j], y[49], t[49], C)
		C, t[50] = madd2(x[j], y[50], t[50], C)
		C, t[51] = madd2(x[j], y[51], t[51], C)
		C, t[52] = madd2(x[j], y[52], t[52], C)
		C, t[53] = madd2(x[j], y[53], t[53], C)
		t[54], D = bits.Add64(t[54], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		C, t[41] = madd2(m, mod[42], t[42], C)
		C, t[42] = madd2(m, mod[43], t[43], C)
		C, t[43] = madd2(m, mod[44], t[44], C)
		C, t[44] = madd2(m, mod[45], t[45], C)
		C, t[45] = madd2(m, mod[46], t[46], C)
		C, t[46] = madd2(m, mod[47], t[47], C)
		C, t[47] = madd2(m, mod[48], t[48], C)
		C, t[48] = madd2(m, mod[49], t[49], C)
		C, t[49] = madd2(m, mod[50], t[50], C)
		C, t[50] = madd2(m, mod[51], t[51], C)
		C, t[51] = madd2(m, mod[52], t[52], C)
		C, t[52] = madd2(m, mod[53], t[53], C)
		t[53], C = bits.Add64(t[54], C, 0)
		t[54], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)
	z[42], D = bits.Sub64(t[42], mod[42], D)
	z[43], D = bits.Sub64(t[43], mod[43], D)
	z[44], D = bits.Sub64(t[44], mod[44], D)
	z[45], D = bits.Sub64(t[45], mod[45], D)
	z[46], D = bits.Sub64(t[46], mod[46], D)
	z[47], D = bits.Sub64(t[47], mod[47], D)
	z[48], D = bits.Sub64(t[48], mod[48], D)
	z[49], D = bits.Sub64(t[49], mod[49], D)
	z[50], D = bits.Sub64(t[50], mod[50], D)
	z[51], D = bits.Sub64(t[51], mod[51], D)
	z[52], D = bits.Sub64(t[52], mod[52], D)
	z[53], D = bits.Sub64(t[53], mod[53], D)

	if D != 0 && t[54] == 0 {
		// reduction was not necessary
		copy(z[:], t[:54])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled3520(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[55]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[55]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[55]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[55]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [56]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC1 = bits.Sub64(mod[42], x[42], gteC1)
	_, gteC1 = bits.Sub64(mod[43], x[43], gteC1)
	_, gteC1 = bits.Sub64(mod[44], x[44], gteC1)
	_, gteC1 = bits.Sub64(mod[45], x[45], gteC1)
	_, gteC1 = bits.Sub64(mod[46], x[46], gteC1)
	_, gteC1 = bits.Sub64(mod[47], x[47], gteC1)
	_, gteC1 = bits.Sub64(mod[48], x[48], gteC1)
	_, gteC1 = bits.Sub64(mod[49], x[49], gteC1)
	_, gteC1 = bits.Sub64(mod[50], x[50], gteC1)
	_, gteC1 = bits.Sub64(mod[51], x[51], gteC1)
	_, gteC1 = bits.Sub64(mod[52], x[52], gteC1)
	_, gteC1 = bits.Sub64(mod[53], x[53], gteC1)
	_, gteC1 = bits.Sub64(mod[54], x[54], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)
	_, gteC2 = bits.Sub64(mod[42], y[42], gteC2)
	_, gteC2 = bits.Sub64(mod[43], y[43], gteC2)
	_, gteC2 = bits.Sub64(mod[44], y[44], gteC2)
	_, gteC2 = bits.Sub64(mod[45], y[45], gteC2)
	_, gteC2 = bits.Sub64(mod[46], y[46], gteC2)
	_, gteC2 = bits.Sub64(mod[47], y[47], gteC2)
	_, gteC2 = bits.Sub64(mod[48], y[48], gteC2)
	_, gteC2 = bits.Sub64(mod[49], y[49], gteC2)
	_, gteC2 = bits.Sub64(mod[50], y[50], gteC2)
	_, gteC2 = bits.Sub64(mod[51], y[51], gteC2)
	_, gteC2 = bits.Sub64(mod[52], y[52], gteC2)
	_, gteC2 = bits.Sub64(mod[53], y[53], gteC2)
	_, gteC2 = bits.Sub64(mod[54], y[54], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)
	C, t[42] = madd1(x[0], y[42], C)
	C, t[43] = madd1(x[0], y[43], C)
	C, t[44] = madd1(x[0], y[44], C)
	C, t[45] = madd1(x[0], y[45], C)
	C, t[46] = madd1(x[0], y[46], C)
	C, t[47] = madd1(x[0], y[47], C)
	C, t[48] = madd1(x[0], y[48], C)
	C, t[49] = madd1(x[0], y[49], C)
	C, t[50] = madd1(x[0], y[50], C)
	C, t[51] = madd1(x[0], y[51], C)
	C, t[52] = madd1(x[0], y[52], C)
	C, t[53] = madd1(x[0], y[53], C)
	C, t[54] = madd1(x[0], y[54], C)

	t[55], D = bits.Add64(t[55], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	C, t[41] = madd2(m, mod[42], t[42], C)
	C, t[42] = madd2(m, mod[43], t[43], C)
	C, t[43] = madd2(m, mod[44], t[44], C)
	C, t[44] = madd2(m, mod[45], t[45], C)
	C, t[45] = madd2(m, mod[46], t[46], C)
	C, t[46] = madd2(m, mod[47], t[47], C)
	C, t[47] = madd2(m, mod[48], t[48], C)
	C, t[48] = madd2(m, mod[49], t[49], C)
	C, t[49] = madd2(m, mod[50], t[50], C)
	C, t[50] = madd2(m, mod[51], t[51], C)
	C, t[51] = madd2(m, mod[52], t[52], C)
	C, t[52] = madd2(m, mod[53], t[53], C)
	C, t[53] = madd2(m, mod[54], t[54], C)
	t[54], C = bits.Add64(t[55], C, 0)
	t[55], _ = bits.Add64(0, D, C)

	for j := 1; j < 55; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		C, t[42] = madd2(x[j], y[42], t[42], C)
		C, t[43] = madd2(x[j], y[43], t[43], C)
		C, t[44] = madd2(x[j], y[44], t[44], C)
		C, t[45] = madd2(x[j], y[45], t[45], C)
		C, t[46] = madd2(x[j], y[46], t[46], C)
		C, t[47] = madd2(x[j], y[47], t[47], C)
		C, t[48] = madd2(x[j], y[48], t[48], C)
		C, t[49] = madd2(x[j], y[49], t[49], C)
		C, t[50] = madd2(x[j], y[50], t[50], C)
		C, t[51] = madd2(x[j], y[51], t[51], C)
		C, t[52] = madd2(x[j], y[52], t[52], C)
		C, t[53] = madd2(x[j], y[53], t[53], C)
		C, t[54] = madd2(x[j], y[54], t[54], C)
		t[55], D = bits.Add64(t[55], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		C, t[41] = madd2(m, mod[42], t[42], C)
		C, t[42] = madd2(m, mod[43], t[43], C)
		C, t[43] = madd2(m, mod[44], t[44], C)
		C, t[44] = madd2(m, mod[45], t[45], C)
		C, t[45] = madd2(m, mod[46], t[46], C)
		C, t[46] = madd2(m, mod[47], t[47], C)
		C, t[47] = madd2(m, mod[48], t[48], C)
		C, t[48] = madd2(m, mod[49], t[49], C)
		C, t[49] = madd2(m, mod[50], t[50], C)
		C, t[50] = madd2(m, mod[51], t[51], C)
		C, t[51] = madd2(m, mod[52], t[52], C)
		C, t[52] = madd2(m, mod[53], t[53], C)
		C, t[53] = madd2(m, mod[54], t[54], C)
		t[54], C = bits.Add64(t[55], C, 0)
		t[55], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)
	z[42], D = bits.Sub64(t[42], mod[42], D)
	z[43], D = bits.Sub64(t[43], mod[43], D)
	z[44], D = bits.Sub64(t[44], mod[44], D)
	z[45], D = bits.Sub64(t[45], mod[45], D)
	z[46], D = bits.Sub64(t[46], mod[46], D)
	z[47], D = bits.Sub64(t[47], mod[47], D)
	z[48], D = bits.Sub64(t[48], mod[48], D)
	z[49], D = bits.Sub64(t[49], mod[49], D)
	z[50], D = bits.Sub64(t[50], mod[50], D)
	z[51], D = bits.Sub64(t[51], mod[51], D)
	z[52], D = bits.Sub64(t[52], mod[52], D)
	z[53], D = bits.Sub64(t[53], mod[53], D)
	z[54], D = bits.Sub64(t[54], mod[54], D)

	if D != 0 && t[55] == 0 {
		// reduction was not necessary
		copy(z[:], t[:55])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled3584(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[56]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[56]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[56]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[56]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [57]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC1 = bits.Sub64(mod[42], x[42], gteC1)
	_, gteC1 = bits.Sub64(mod[43], x[43], gteC1)
	_, gteC1 = bits.Sub64(mod[44], x[44], gteC1)
	_, gteC1 = bits.Sub64(mod[45], x[45], gteC1)
	_, gteC1 = bits.Sub64(mod[46], x[46], gteC1)
	_, gteC1 = bits.Sub64(mod[47], x[47], gteC1)
	_, gteC1 = bits.Sub64(mod[48], x[48], gteC1)
	_, gteC1 = bits.Sub64(mod[49], x[49], gteC1)
	_, gteC1 = bits.Sub64(mod[50], x[50], gteC1)
	_, gteC1 = bits.Sub64(mod[51], x[51], gteC1)
	_, gteC1 = bits.Sub64(mod[52], x[52], gteC1)
	_, gteC1 = bits.Sub64(mod[53], x[53], gteC1)
	_, gteC1 = bits.Sub64(mod[54], x[54], gteC1)
	_, gteC1 = bits.Sub64(mod[55], x[55], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)
	_, gteC2 = bits.Sub64(mod[42], y[42], gteC2)
	_, gteC2 = bits.Sub64(mod[43], y[43], gteC2)
	_, gteC2 = bits.Sub64(mod[44], y[44], gteC2)
	_, gteC2 = bits.Sub64(mod[45], y[45], gteC2)
	_, gteC2 = bits.Sub64(mod[46], y[46], gteC2)
	_, gteC2 = bits.Sub64(mod[47], y[47], gteC2)
	_, gteC2 = bits.Sub64(mod[48], y[48], gteC2)
	_, gteC2 = bits.Sub64(mod[49], y[49], gteC2)
	_, gteC2 = bits.Sub64(mod[50], y[50], gteC2)
	_, gteC2 = bits.Sub64(mod[51], y[51], gteC2)
	_, gteC2 = bits.Sub64(mod[52], y[52], gteC2)
	_, gteC2 = bits.Sub64(mod[53], y[53], gteC2)
	_, gteC2 = bits.Sub64(mod[54], y[54], gteC2)
	_, gteC2 = bits.Sub64(mod[55], y[55], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)
	C, t[42] = madd1(x[0], y[42], C)
	C, t[43] = madd1(x[0], y[43], C)
	C, t[44] = madd1(x[0], y[44], C)
	C, t[45] = madd1(x[0], y[45], C)
	C, t[46] = madd1(x[0], y[46], C)
	C, t[47] = madd1(x[0], y[47], C)
	C, t[48] = madd1(x[0], y[48], C)
	C, t[49] = madd1(x[0], y[49], C)
	C, t[50] = madd1(x[0], y[50], C)
	C, t[51] = madd1(x[0], y[51], C)
	C, t[52] = madd1(x[0], y[52], C)
	C, t[53] = madd1(x[0], y[53], C)
	C, t[54] = madd1(x[0], y[54], C)
	C, t[55] = madd1(x[0], y[55], C)

	t[56], D = bits.Add64(t[56], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	C, t[41] = madd2(m, mod[42], t[42], C)
	C, t[42] = madd2(m, mod[43], t[43], C)
	C, t[43] = madd2(m, mod[44], t[44], C)
	C, t[44] = madd2(m, mod[45], t[45], C)
	C, t[45] = madd2(m, mod[46], t[46], C)
	C, t[46] = madd2(m, mod[47], t[47], C)
	C, t[47] = madd2(m, mod[48], t[48], C)
	C, t[48] = madd2(m, mod[49], t[49], C)
	C, t[49] = madd2(m, mod[50], t[50], C)
	C, t[50] = madd2(m, mod[51], t[51], C)
	C, t[51] = madd2(m, mod[52], t[52], C)
	C, t[52] = madd2(m, mod[53], t[53], C)
	C, t[53] = madd2(m, mod[54], t[54], C)
	C, t[54] = madd2(m, mod[55], t[55], C)
	t[55], C = bits.Add64(t[56], C, 0)
	t[56], _ = bits.Add64(0, D, C)

	for j := 1; j < 56; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		C, t[42] = madd2(x[j], y[42], t[42], C)
		C, t[43] = madd2(x[j], y[43], t[43], C)
		C, t[44] = madd2(x[j], y[44], t[44], C)
		C, t[45] = madd2(x[j], y[45], t[45], C)
		C, t[46] = madd2(x[j], y[46], t[46], C)
		C, t[47] = madd2(x[j], y[47], t[47], C)
		C, t[48] = madd2(x[j], y[48], t[48], C)
		C, t[49] = madd2(x[j], y[49], t[49], C)
		C, t[50] = madd2(x[j], y[50], t[50], C)
		C, t[51] = madd2(x[j], y[51], t[51], C)
		C, t[52] = madd2(x[j], y[52], t[52], C)
		C, t[53] = madd2(x[j], y[53], t[53], C)
		C, t[54] = madd2(x[j], y[54], t[54], C)
		C, t[55] = madd2(x[j], y[55], t[55], C)
		t[56], D = bits.Add64(t[56], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		C, t[41] = madd2(m, mod[42], t[42], C)
		C, t[42] = madd2(m, mod[43], t[43], C)
		C, t[43] = madd2(m, mod[44], t[44], C)
		C, t[44] = madd2(m, mod[45], t[45], C)
		C, t[45] = madd2(m, mod[46], t[46], C)
		C, t[46] = madd2(m, mod[47], t[47], C)
		C, t[47] = madd2(m, mod[48], t[48], C)
		C, t[48] = madd2(m, mod[49], t[49], C)
		C, t[49] = madd2(m, mod[50], t[50], C)
		C, t[50] = madd2(m, mod[51], t[51], C)
		C, t[51] = madd2(m, mod[52], t[52], C)
		C, t[52] = madd2(m, mod[53], t[53], C)
		C, t[53] = madd2(m, mod[54], t[54], C)
		C, t[54] = madd2(m, mod[55], t[55], C)
		t[55], C = bits.Add64(t[56], C, 0)
		t[56], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)
	z[42], D = bits.Sub64(t[42], mod[42], D)
	z[43], D = bits.Sub64(t[43], mod[43], D)
	z[44], D = bits.Sub64(t[44], mod[44], D)
	z[45], D = bits.Sub64(t[45], mod[45], D)
	z[46], D = bits.Sub64(t[46], mod[46], D)
	z[47], D = bits.Sub64(t[47], mod[47], D)
	z[48], D = bits.Sub64(t[48], mod[48], D)
	z[49], D = bits.Sub64(t[49], mod[49], D)
	z[50], D = bits.Sub64(t[50], mod[50], D)
	z[51], D = bits.Sub64(t[51], mod[51], D)
	z[52], D = bits.Sub64(t[52], mod[52], D)
	z[53], D = bits.Sub64(t[53], mod[53], D)
	z[54], D = bits.Sub64(t[54], mod[54], D)
	z[55], D = bits.Sub64(t[55], mod[55], D)

	if D != 0 && t[56] == 0 {
		// reduction was not necessary
		copy(z[:], t[:56])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled3648(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[57]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[57]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[57]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[57]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [58]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC1 = bits.Sub64(mod[42], x[42], gteC1)
	_, gteC1 = bits.Sub64(mod[43], x[43], gteC1)
	_, gteC1 = bits.Sub64(mod[44], x[44], gteC1)
	_, gteC1 = bits.Sub64(mod[45], x[45], gteC1)
	_, gteC1 = bits.Sub64(mod[46], x[46], gteC1)
	_, gteC1 = bits.Sub64(mod[47], x[47], gteC1)
	_, gteC1 = bits.Sub64(mod[48], x[48], gteC1)
	_, gteC1 = bits.Sub64(mod[49], x[49], gteC1)
	_, gteC1 = bits.Sub64(mod[50], x[50], gteC1)
	_, gteC1 = bits.Sub64(mod[51], x[51], gteC1)
	_, gteC1 = bits.Sub64(mod[52], x[52], gteC1)
	_, gteC1 = bits.Sub64(mod[53], x[53], gteC1)
	_, gteC1 = bits.Sub64(mod[54], x[54], gteC1)
	_, gteC1 = bits.Sub64(mod[55], x[55], gteC1)
	_, gteC1 = bits.Sub64(mod[56], x[56], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)
	_, gteC2 = bits.Sub64(mod[42], y[42], gteC2)
	_, gteC2 = bits.Sub64(mod[43], y[43], gteC2)
	_, gteC2 = bits.Sub64(mod[44], y[44], gteC2)
	_, gteC2 = bits.Sub64(mod[45], y[45], gteC2)
	_, gteC2 = bits.Sub64(mod[46], y[46], gteC2)
	_, gteC2 = bits.Sub64(mod[47], y[47], gteC2)
	_, gteC2 = bits.Sub64(mod[48], y[48], gteC2)
	_, gteC2 = bits.Sub64(mod[49], y[49], gteC2)
	_, gteC2 = bits.Sub64(mod[50], y[50], gteC2)
	_, gteC2 = bits.Sub64(mod[51], y[51], gteC2)
	_, gteC2 = bits.Sub64(mod[52], y[52], gteC2)
	_, gteC2 = bits.Sub64(mod[53], y[53], gteC2)
	_, gteC2 = bits.Sub64(mod[54], y[54], gteC2)
	_, gteC2 = bits.Sub64(mod[55], y[55], gteC2)
	_, gteC2 = bits.Sub64(mod[56], y[56], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)
	C, t[42] = madd1(x[0], y[42], C)
	C, t[43] = madd1(x[0], y[43], C)
	C, t[44] = madd1(x[0], y[44], C)
	C, t[45] = madd1(x[0], y[45], C)
	C, t[46] = madd1(x[0], y[46], C)
	C, t[47] = madd1(x[0], y[47], C)
	C, t[48] = madd1(x[0], y[48], C)
	C, t[49] = madd1(x[0], y[49], C)
	C, t[50] = madd1(x[0], y[50], C)
	C, t[51] = madd1(x[0], y[51], C)
	C, t[52] = madd1(x[0], y[52], C)
	C, t[53] = madd1(x[0], y[53], C)
	C, t[54] = madd1(x[0], y[54], C)
	C, t[55] = madd1(x[0], y[55], C)
	C, t[56] = madd1(x[0], y[56], C)

	t[57], D = bits.Add64(t[57], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	C, t[41] = madd2(m, mod[42], t[42], C)
	C, t[42] = madd2(m, mod[43], t[43], C)
	C, t[43] = madd2(m, mod[44], t[44], C)
	C, t[44] = madd2(m, mod[45], t[45], C)
	C, t[45] = madd2(m, mod[46], t[46], C)
	C, t[46] = madd2(m, mod[47], t[47], C)
	C, t[47] = madd2(m, mod[48], t[48], C)
	C, t[48] = madd2(m, mod[49], t[49], C)
	C, t[49] = madd2(m, mod[50], t[50], C)
	C, t[50] = madd2(m, mod[51], t[51], C)
	C, t[51] = madd2(m, mod[52], t[52], C)
	C, t[52] = madd2(m, mod[53], t[53], C)
	C, t[53] = madd2(m, mod[54], t[54], C)
	C, t[54] = madd2(m, mod[55], t[55], C)
	C, t[55] = madd2(m, mod[56], t[56], C)
	t[56], C = bits.Add64(t[57], C, 0)
	t[57], _ = bits.Add64(0, D, C)

	for j := 1; j < 57; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		C, t[42] = madd2(x[j], y[42], t[42], C)
		C, t[43] = madd2(x[j], y[43], t[43], C)
		C, t[44] = madd2(x[j], y[44], t[44], C)
		C, t[45] = madd2(x[j], y[45], t[45], C)
		C, t[46] = madd2(x[j], y[46], t[46], C)
		C, t[47] = madd2(x[j], y[47], t[47], C)
		C, t[48] = madd2(x[j], y[48], t[48], C)
		C, t[49] = madd2(x[j], y[49], t[49], C)
		C, t[50] = madd2(x[j], y[50], t[50], C)
		C, t[51] = madd2(x[j], y[51], t[51], C)
		C, t[52] = madd2(x[j], y[52], t[52], C)
		C, t[53] = madd2(x[j], y[53], t[53], C)
		C, t[54] = madd2(x[j], y[54], t[54], C)
		C, t[55] = madd2(x[j], y[55], t[55], C)
		C, t[56] = madd2(x[j], y[56], t[56], C)
		t[57], D = bits.Add64(t[57], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		C, t[41] = madd2(m, mod[42], t[42], C)
		C, t[42] = madd2(m, mod[43], t[43], C)
		C, t[43] = madd2(m, mod[44], t[44], C)
		C, t[44] = madd2(m, mod[45], t[45], C)
		C, t[45] = madd2(m, mod[46], t[46], C)
		C, t[46] = madd2(m, mod[47], t[47], C)
		C, t[47] = madd2(m, mod[48], t[48], C)
		C, t[48] = madd2(m, mod[49], t[49], C)
		C, t[49] = madd2(m, mod[50], t[50], C)
		C, t[50] = madd2(m, mod[51], t[51], C)
		C, t[51] = madd2(m, mod[52], t[52], C)
		C, t[52] = madd2(m, mod[53], t[53], C)
		C, t[53] = madd2(m, mod[54], t[54], C)
		C, t[54] = madd2(m, mod[55], t[55], C)
		C, t[55] = madd2(m, mod[56], t[56], C)
		t[56], C = bits.Add64(t[57], C, 0)
		t[57], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)
	z[42], D = bits.Sub64(t[42], mod[42], D)
	z[43], D = bits.Sub64(t[43], mod[43], D)
	z[44], D = bits.Sub64(t[44], mod[44], D)
	z[45], D = bits.Sub64(t[45], mod[45], D)
	z[46], D = bits.Sub64(t[46], mod[46], D)
	z[47], D = bits.Sub64(t[47], mod[47], D)
	z[48], D = bits.Sub64(t[48], mod[48], D)
	z[49], D = bits.Sub64(t[49], mod[49], D)
	z[50], D = bits.Sub64(t[50], mod[50], D)
	z[51], D = bits.Sub64(t[51], mod[51], D)
	z[52], D = bits.Sub64(t[52], mod[52], D)
	z[53], D = bits.Sub64(t[53], mod[53], D)
	z[54], D = bits.Sub64(t[54], mod[54], D)
	z[55], D = bits.Sub64(t[55], mod[55], D)
	z[56], D = bits.Sub64(t[56], mod[56], D)

	if D != 0 && t[57] == 0 {
		// reduction was not necessary
		copy(z[:], t[:57])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled3712(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[58]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[58]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[58]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[58]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [59]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC1 = bits.Sub64(mod[42], x[42], gteC1)
	_, gteC1 = bits.Sub64(mod[43], x[43], gteC1)
	_, gteC1 = bits.Sub64(mod[44], x[44], gteC1)
	_, gteC1 = bits.Sub64(mod[45], x[45], gteC1)
	_, gteC1 = bits.Sub64(mod[46], x[46], gteC1)
	_, gteC1 = bits.Sub64(mod[47], x[47], gteC1)
	_, gteC1 = bits.Sub64(mod[48], x[48], gteC1)
	_, gteC1 = bits.Sub64(mod[49], x[49], gteC1)
	_, gteC1 = bits.Sub64(mod[50], x[50], gteC1)
	_, gteC1 = bits.Sub64(mod[51], x[51], gteC1)
	_, gteC1 = bits.Sub64(mod[52], x[52], gteC1)
	_, gteC1 = bits.Sub64(mod[53], x[53], gteC1)
	_, gteC1 = bits.Sub64(mod[54], x[54], gteC1)
	_, gteC1 = bits.Sub64(mod[55], x[55], gteC1)
	_, gteC1 = bits.Sub64(mod[56], x[56], gteC1)
	_, gteC1 = bits.Sub64(mod[57], x[57], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)
	_, gteC2 = bits.Sub64(mod[42], y[42], gteC2)
	_, gteC2 = bits.Sub64(mod[43], y[43], gteC2)
	_, gteC2 = bits.Sub64(mod[44], y[44], gteC2)
	_, gteC2 = bits.Sub64(mod[45], y[45], gteC2)
	_, gteC2 = bits.Sub64(mod[46], y[46], gteC2)
	_, gteC2 = bits.Sub64(mod[47], y[47], gteC2)
	_, gteC2 = bits.Sub64(mod[48], y[48], gteC2)
	_, gteC2 = bits.Sub64(mod[49], y[49], gteC2)
	_, gteC2 = bits.Sub64(mod[50], y[50], gteC2)
	_, gteC2 = bits.Sub64(mod[51], y[51], gteC2)
	_, gteC2 = bits.Sub64(mod[52], y[52], gteC2)
	_, gteC2 = bits.Sub64(mod[53], y[53], gteC2)
	_, gteC2 = bits.Sub64(mod[54], y[54], gteC2)
	_, gteC2 = bits.Sub64(mod[55], y[55], gteC2)
	_, gteC2 = bits.Sub64(mod[56], y[56], gteC2)
	_, gteC2 = bits.Sub64(mod[57], y[57], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)
	C, t[42] = madd1(x[0], y[42], C)
	C, t[43] = madd1(x[0], y[43], C)
	C, t[44] = madd1(x[0], y[44], C)
	C, t[45] = madd1(x[0], y[45], C)
	C, t[46] = madd1(x[0], y[46], C)
	C, t[47] = madd1(x[0], y[47], C)
	C, t[48] = madd1(x[0], y[48], C)
	C, t[49] = madd1(x[0], y[49], C)
	C, t[50] = madd1(x[0], y[50], C)
	C, t[51] = madd1(x[0], y[51], C)
	C, t[52] = madd1(x[0], y[52], C)
	C, t[53] = madd1(x[0], y[53], C)
	C, t[54] = madd1(x[0], y[54], C)
	C, t[55] = madd1(x[0], y[55], C)
	C, t[56] = madd1(x[0], y[56], C)
	C, t[57] = madd1(x[0], y[57], C)

	t[58], D = bits.Add64(t[58], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	C, t[41] = madd2(m, mod[42], t[42], C)
	C, t[42] = madd2(m, mod[43], t[43], C)
	C, t[43] = madd2(m, mod[44], t[44], C)
	C, t[44] = madd2(m, mod[45], t[45], C)
	C, t[45] = madd2(m, mod[46], t[46], C)
	C, t[46] = madd2(m, mod[47], t[47], C)
	C, t[47] = madd2(m, mod[48], t[48], C)
	C, t[48] = madd2(m, mod[49], t[49], C)
	C, t[49] = madd2(m, mod[50], t[50], C)
	C, t[50] = madd2(m, mod[51], t[51], C)
	C, t[51] = madd2(m, mod[52], t[52], C)
	C, t[52] = madd2(m, mod[53], t[53], C)
	C, t[53] = madd2(m, mod[54], t[54], C)
	C, t[54] = madd2(m, mod[55], t[55], C)
	C, t[55] = madd2(m, mod[56], t[56], C)
	C, t[56] = madd2(m, mod[57], t[57], C)
	t[57], C = bits.Add64(t[58], C, 0)
	t[58], _ = bits.Add64(0, D, C)

	for j := 1; j < 58; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		C, t[42] = madd2(x[j], y[42], t[42], C)
		C, t[43] = madd2(x[j], y[43], t[43], C)
		C, t[44] = madd2(x[j], y[44], t[44], C)
		C, t[45] = madd2(x[j], y[45], t[45], C)
		C, t[46] = madd2(x[j], y[46], t[46], C)
		C, t[47] = madd2(x[j], y[47], t[47], C)
		C, t[48] = madd2(x[j], y[48], t[48], C)
		C, t[49] = madd2(x[j], y[49], t[49], C)
		C, t[50] = madd2(x[j], y[50], t[50], C)
		C, t[51] = madd2(x[j], y[51], t[51], C)
		C, t[52] = madd2(x[j], y[52], t[52], C)
		C, t[53] = madd2(x[j], y[53], t[53], C)
		C, t[54] = madd2(x[j], y[54], t[54], C)
		C, t[55] = madd2(x[j], y[55], t[55], C)
		C, t[56] = madd2(x[j], y[56], t[56], C)
		C, t[57] = madd2(x[j], y[57], t[57], C)
		t[58], D = bits.Add64(t[58], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		C, t[41] = madd2(m, mod[42], t[42], C)
		C, t[42] = madd2(m, mod[43], t[43], C)
		C, t[43] = madd2(m, mod[44], t[44], C)
		C, t[44] = madd2(m, mod[45], t[45], C)
		C, t[45] = madd2(m, mod[46], t[46], C)
		C, t[46] = madd2(m, mod[47], t[47], C)
		C, t[47] = madd2(m, mod[48], t[48], C)
		C, t[48] = madd2(m, mod[49], t[49], C)
		C, t[49] = madd2(m, mod[50], t[50], C)
		C, t[50] = madd2(m, mod[51], t[51], C)
		C, t[51] = madd2(m, mod[52], t[52], C)
		C, t[52] = madd2(m, mod[53], t[53], C)
		C, t[53] = madd2(m, mod[54], t[54], C)
		C, t[54] = madd2(m, mod[55], t[55], C)
		C, t[55] = madd2(m, mod[56], t[56], C)
		C, t[56] = madd2(m, mod[57], t[57], C)
		t[57], C = bits.Add64(t[58], C, 0)
		t[58], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)
	z[42], D = bits.Sub64(t[42], mod[42], D)
	z[43], D = bits.Sub64(t[43], mod[43], D)
	z[44], D = bits.Sub64(t[44], mod[44], D)
	z[45], D = bits.Sub64(t[45], mod[45], D)
	z[46], D = bits.Sub64(t[46], mod[46], D)
	z[47], D = bits.Sub64(t[47], mod[47], D)
	z[48], D = bits.Sub64(t[48], mod[48], D)
	z[49], D = bits.Sub64(t[49], mod[49], D)
	z[50], D = bits.Sub64(t[50], mod[50], D)
	z[51], D = bits.Sub64(t[51], mod[51], D)
	z[52], D = bits.Sub64(t[52], mod[52], D)
	z[53], D = bits.Sub64(t[53], mod[53], D)
	z[54], D = bits.Sub64(t[54], mod[54], D)
	z[55], D = bits.Sub64(t[55], mod[55], D)
	z[56], D = bits.Sub64(t[56], mod[56], D)
	z[57], D = bits.Sub64(t[57], mod[57], D)

	if D != 0 && t[58] == 0 {
		// reduction was not necessary
		copy(z[:], t[:58])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled3776(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[59]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[59]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[59]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[59]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [60]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC1 = bits.Sub64(mod[42], x[42], gteC1)
	_, gteC1 = bits.Sub64(mod[43], x[43], gteC1)
	_, gteC1 = bits.Sub64(mod[44], x[44], gteC1)
	_, gteC1 = bits.Sub64(mod[45], x[45], gteC1)
	_, gteC1 = bits.Sub64(mod[46], x[46], gteC1)
	_, gteC1 = bits.Sub64(mod[47], x[47], gteC1)
	_, gteC1 = bits.Sub64(mod[48], x[48], gteC1)
	_, gteC1 = bits.Sub64(mod[49], x[49], gteC1)
	_, gteC1 = bits.Sub64(mod[50], x[50], gteC1)
	_, gteC1 = bits.Sub64(mod[51], x[51], gteC1)
	_, gteC1 = bits.Sub64(mod[52], x[52], gteC1)
	_, gteC1 = bits.Sub64(mod[53], x[53], gteC1)
	_, gteC1 = bits.Sub64(mod[54], x[54], gteC1)
	_, gteC1 = bits.Sub64(mod[55], x[55], gteC1)
	_, gteC1 = bits.Sub64(mod[56], x[56], gteC1)
	_, gteC1 = bits.Sub64(mod[57], x[57], gteC1)
	_, gteC1 = bits.Sub64(mod[58], x[58], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)
	_, gteC2 = bits.Sub64(mod[42], y[42], gteC2)
	_, gteC2 = bits.Sub64(mod[43], y[43], gteC2)
	_, gteC2 = bits.Sub64(mod[44], y[44], gteC2)
	_, gteC2 = bits.Sub64(mod[45], y[45], gteC2)
	_, gteC2 = bits.Sub64(mod[46], y[46], gteC2)
	_, gteC2 = bits.Sub64(mod[47], y[47], gteC2)
	_, gteC2 = bits.Sub64(mod[48], y[48], gteC2)
	_, gteC2 = bits.Sub64(mod[49], y[49], gteC2)
	_, gteC2 = bits.Sub64(mod[50], y[50], gteC2)
	_, gteC2 = bits.Sub64(mod[51], y[51], gteC2)
	_, gteC2 = bits.Sub64(mod[52], y[52], gteC2)
	_, gteC2 = bits.Sub64(mod[53], y[53], gteC2)
	_, gteC2 = bits.Sub64(mod[54], y[54], gteC2)
	_, gteC2 = bits.Sub64(mod[55], y[55], gteC2)
	_, gteC2 = bits.Sub64(mod[56], y[56], gteC2)
	_, gteC2 = bits.Sub64(mod[57], y[57], gteC2)
	_, gteC2 = bits.Sub64(mod[58], y[58], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)
	C, t[42] = madd1(x[0], y[42], C)
	C, t[43] = madd1(x[0], y[43], C)
	C, t[44] = madd1(x[0], y[44], C)
	C, t[45] = madd1(x[0], y[45], C)
	C, t[46] = madd1(x[0], y[46], C)
	C, t[47] = madd1(x[0], y[47], C)
	C, t[48] = madd1(x[0], y[48], C)
	C, t[49] = madd1(x[0], y[49], C)
	C, t[50] = madd1(x[0], y[50], C)
	C, t[51] = madd1(x[0], y[51], C)
	C, t[52] = madd1(x[0], y[52], C)
	C, t[53] = madd1(x[0], y[53], C)
	C, t[54] = madd1(x[0], y[54], C)
	C, t[55] = madd1(x[0], y[55], C)
	C, t[56] = madd1(x[0], y[56], C)
	C, t[57] = madd1(x[0], y[57], C)
	C, t[58] = madd1(x[0], y[58], C)

	t[59], D = bits.Add64(t[59], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	C, t[41] = madd2(m, mod[42], t[42], C)
	C, t[42] = madd2(m, mod[43], t[43], C)
	C, t[43] = madd2(m, mod[44], t[44], C)
	C, t[44] = madd2(m, mod[45], t[45], C)
	C, t[45] = madd2(m, mod[46], t[46], C)
	C, t[46] = madd2(m, mod[47], t[47], C)
	C, t[47] = madd2(m, mod[48], t[48], C)
	C, t[48] = madd2(m, mod[49], t[49], C)
	C, t[49] = madd2(m, mod[50], t[50], C)
	C, t[50] = madd2(m, mod[51], t[51], C)
	C, t[51] = madd2(m, mod[52], t[52], C)
	C, t[52] = madd2(m, mod[53], t[53], C)
	C, t[53] = madd2(m, mod[54], t[54], C)
	C, t[54] = madd2(m, mod[55], t[55], C)
	C, t[55] = madd2(m, mod[56], t[56], C)
	C, t[56] = madd2(m, mod[57], t[57], C)
	C, t[57] = madd2(m, mod[58], t[58], C)
	t[58], C = bits.Add64(t[59], C, 0)
	t[59], _ = bits.Add64(0, D, C)

	for j := 1; j < 59; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		C, t[42] = madd2(x[j], y[42], t[42], C)
		C, t[43] = madd2(x[j], y[43], t[43], C)
		C, t[44] = madd2(x[j], y[44], t[44], C)
		C, t[45] = madd2(x[j], y[45], t[45], C)
		C, t[46] = madd2(x[j], y[46], t[46], C)
		C, t[47] = madd2(x[j], y[47], t[47], C)
		C, t[48] = madd2(x[j], y[48], t[48], C)
		C, t[49] = madd2(x[j], y[49], t[49], C)
		C, t[50] = madd2(x[j], y[50], t[50], C)
		C, t[51] = madd2(x[j], y[51], t[51], C)
		C, t[52] = madd2(x[j], y[52], t[52], C)
		C, t[53] = madd2(x[j], y[53], t[53], C)
		C, t[54] = madd2(x[j], y[54], t[54], C)
		C, t[55] = madd2(x[j], y[55], t[55], C)
		C, t[56] = madd2(x[j], y[56], t[56], C)
		C, t[57] = madd2(x[j], y[57], t[57], C)
		C, t[58] = madd2(x[j], y[58], t[58], C)
		t[59], D = bits.Add64(t[59], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		C, t[41] = madd2(m, mod[42], t[42], C)
		C, t[42] = madd2(m, mod[43], t[43], C)
		C, t[43] = madd2(m, mod[44], t[44], C)
		C, t[44] = madd2(m, mod[45], t[45], C)
		C, t[45] = madd2(m, mod[46], t[46], C)
		C, t[46] = madd2(m, mod[47], t[47], C)
		C, t[47] = madd2(m, mod[48], t[48], C)
		C, t[48] = madd2(m, mod[49], t[49], C)
		C, t[49] = madd2(m, mod[50], t[50], C)
		C, t[50] = madd2(m, mod[51], t[51], C)
		C, t[51] = madd2(m, mod[52], t[52], C)
		C, t[52] = madd2(m, mod[53], t[53], C)
		C, t[53] = madd2(m, mod[54], t[54], C)
		C, t[54] = madd2(m, mod[55], t[55], C)
		C, t[55] = madd2(m, mod[56], t[56], C)
		C, t[56] = madd2(m, mod[57], t[57], C)
		C, t[57] = madd2(m, mod[58], t[58], C)
		t[58], C = bits.Add64(t[59], C, 0)
		t[59], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)
	z[42], D = bits.Sub64(t[42], mod[42], D)
	z[43], D = bits.Sub64(t[43], mod[43], D)
	z[44], D = bits.Sub64(t[44], mod[44], D)
	z[45], D = bits.Sub64(t[45], mod[45], D)
	z[46], D = bits.Sub64(t[46], mod[46], D)
	z[47], D = bits.Sub64(t[47], mod[47], D)
	z[48], D = bits.Sub64(t[48], mod[48], D)
	z[49], D = bits.Sub64(t[49], mod[49], D)
	z[50], D = bits.Sub64(t[50], mod[50], D)
	z[51], D = bits.Sub64(t[51], mod[51], D)
	z[52], D = bits.Sub64(t[52], mod[52], D)
	z[53], D = bits.Sub64(t[53], mod[53], D)
	z[54], D = bits.Sub64(t[54], mod[54], D)
	z[55], D = bits.Sub64(t[55], mod[55], D)
	z[56], D = bits.Sub64(t[56], mod[56], D)
	z[57], D = bits.Sub64(t[57], mod[57], D)
	z[58], D = bits.Sub64(t[58], mod[58], D)

	if D != 0 && t[59] == 0 {
		// reduction was not necessary
		copy(z[:], t[:59])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled3840(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[60]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[60]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[60]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[60]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [61]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC1 = bits.Sub64(mod[42], x[42], gteC1)
	_, gteC1 = bits.Sub64(mod[43], x[43], gteC1)
	_, gteC1 = bits.Sub64(mod[44], x[44], gteC1)
	_, gteC1 = bits.Sub64(mod[45], x[45], gteC1)
	_, gteC1 = bits.Sub64(mod[46], x[46], gteC1)
	_, gteC1 = bits.Sub64(mod[47], x[47], gteC1)
	_, gteC1 = bits.Sub64(mod[48], x[48], gteC1)
	_, gteC1 = bits.Sub64(mod[49], x[49], gteC1)
	_, gteC1 = bits.Sub64(mod[50], x[50], gteC1)
	_, gteC1 = bits.Sub64(mod[51], x[51], gteC1)
	_, gteC1 = bits.Sub64(mod[52], x[52], gteC1)
	_, gteC1 = bits.Sub64(mod[53], x[53], gteC1)
	_, gteC1 = bits.Sub64(mod[54], x[54], gteC1)
	_, gteC1 = bits.Sub64(mod[55], x[55], gteC1)
	_, gteC1 = bits.Sub64(mod[56], x[56], gteC1)
	_, gteC1 = bits.Sub64(mod[57], x[57], gteC1)
	_, gteC1 = bits.Sub64(mod[58], x[58], gteC1)
	_, gteC1 = bits.Sub64(mod[59], x[59], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)
	_, gteC2 = bits.Sub64(mod[42], y[42], gteC2)
	_, gteC2 = bits.Sub64(mod[43], y[43], gteC2)
	_, gteC2 = bits.Sub64(mod[44], y[44], gteC2)
	_, gteC2 = bits.Sub64(mod[45], y[45], gteC2)
	_, gteC2 = bits.Sub64(mod[46], y[46], gteC2)
	_, gteC2 = bits.Sub64(mod[47], y[47], gteC2)
	_, gteC2 = bits.Sub64(mod[48], y[48], gteC2)
	_, gteC2 = bits.Sub64(mod[49], y[49], gteC2)
	_, gteC2 = bits.Sub64(mod[50], y[50], gteC2)
	_, gteC2 = bits.Sub64(mod[51], y[51], gteC2)
	_, gteC2 = bits.Sub64(mod[52], y[52], gteC2)
	_, gteC2 = bits.Sub64(mod[53], y[53], gteC2)
	_, gteC2 = bits.Sub64(mod[54], y[54], gteC2)
	_, gteC2 = bits.Sub64(mod[55], y[55], gteC2)
	_, gteC2 = bits.Sub64(mod[56], y[56], gteC2)
	_, gteC2 = bits.Sub64(mod[57], y[57], gteC2)
	_, gteC2 = bits.Sub64(mod[58], y[58], gteC2)
	_, gteC2 = bits.Sub64(mod[59], y[59], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)
	C, t[42] = madd1(x[0], y[42], C)
	C, t[43] = madd1(x[0], y[43], C)
	C, t[44] = madd1(x[0], y[44], C)
	C, t[45] = madd1(x[0], y[45], C)
	C, t[46] = madd1(x[0], y[46], C)
	C, t[47] = madd1(x[0], y[47], C)
	C, t[48] = madd1(x[0], y[48], C)
	C, t[49] = madd1(x[0], y[49], C)
	C, t[50] = madd1(x[0], y[50], C)
	C, t[51] = madd1(x[0], y[51], C)
	C, t[52] = madd1(x[0], y[52], C)
	C, t[53] = madd1(x[0], y[53], C)
	C, t[54] = madd1(x[0], y[54], C)
	C, t[55] = madd1(x[0], y[55], C)
	C, t[56] = madd1(x[0], y[56], C)
	C, t[57] = madd1(x[0], y[57], C)
	C, t[58] = madd1(x[0], y[58], C)
	C, t[59] = madd1(x[0], y[59], C)

	t[60], D = bits.Add64(t[60], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	C, t[41] = madd2(m, mod[42], t[42], C)
	C, t[42] = madd2(m, mod[43], t[43], C)
	C, t[43] = madd2(m, mod[44], t[44], C)
	C, t[44] = madd2(m, mod[45], t[45], C)
	C, t[45] = madd2(m, mod[46], t[46], C)
	C, t[46] = madd2(m, mod[47], t[47], C)
	C, t[47] = madd2(m, mod[48], t[48], C)
	C, t[48] = madd2(m, mod[49], t[49], C)
	C, t[49] = madd2(m, mod[50], t[50], C)
	C, t[50] = madd2(m, mod[51], t[51], C)
	C, t[51] = madd2(m, mod[52], t[52], C)
	C, t[52] = madd2(m, mod[53], t[53], C)
	C, t[53] = madd2(m, mod[54], t[54], C)
	C, t[54] = madd2(m, mod[55], t[55], C)
	C, t[55] = madd2(m, mod[56], t[56], C)
	C, t[56] = madd2(m, mod[57], t[57], C)
	C, t[57] = madd2(m, mod[58], t[58], C)
	C, t[58] = madd2(m, mod[59], t[59], C)
	t[59], C = bits.Add64(t[60], C, 0)
	t[60], _ = bits.Add64(0, D, C)

	for j := 1; j < 60; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		C, t[42] = madd2(x[j], y[42], t[42], C)
		C, t[43] = madd2(x[j], y[43], t[43], C)
		C, t[44] = madd2(x[j], y[44], t[44], C)
		C, t[45] = madd2(x[j], y[45], t[45], C)
		C, t[46] = madd2(x[j], y[46], t[46], C)
		C, t[47] = madd2(x[j], y[47], t[47], C)
		C, t[48] = madd2(x[j], y[48], t[48], C)
		C, t[49] = madd2(x[j], y[49], t[49], C)
		C, t[50] = madd2(x[j], y[50], t[50], C)
		C, t[51] = madd2(x[j], y[51], t[51], C)
		C, t[52] = madd2(x[j], y[52], t[52], C)
		C, t[53] = madd2(x[j], y[53], t[53], C)
		C, t[54] = madd2(x[j], y[54], t[54], C)
		C, t[55] = madd2(x[j], y[55], t[55], C)
		C, t[56] = madd2(x[j], y[56], t[56], C)
		C, t[57] = madd2(x[j], y[57], t[57], C)
		C, t[58] = madd2(x[j], y[58], t[58], C)
		C, t[59] = madd2(x[j], y[59], t[59], C)
		t[60], D = bits.Add64(t[60], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		C, t[41] = madd2(m, mod[42], t[42], C)
		C, t[42] = madd2(m, mod[43], t[43], C)
		C, t[43] = madd2(m, mod[44], t[44], C)
		C, t[44] = madd2(m, mod[45], t[45], C)
		C, t[45] = madd2(m, mod[46], t[46], C)
		C, t[46] = madd2(m, mod[47], t[47], C)
		C, t[47] = madd2(m, mod[48], t[48], C)
		C, t[48] = madd2(m, mod[49], t[49], C)
		C, t[49] = madd2(m, mod[50], t[50], C)
		C, t[50] = madd2(m, mod[51], t[51], C)
		C, t[51] = madd2(m, mod[52], t[52], C)
		C, t[52] = madd2(m, mod[53], t[53], C)
		C, t[53] = madd2(m, mod[54], t[54], C)
		C, t[54] = madd2(m, mod[55], t[55], C)
		C, t[55] = madd2(m, mod[56], t[56], C)
		C, t[56] = madd2(m, mod[57], t[57], C)
		C, t[57] = madd2(m, mod[58], t[58], C)
		C, t[58] = madd2(m, mod[59], t[59], C)
		t[59], C = bits.Add64(t[60], C, 0)
		t[60], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)
	z[42], D = bits.Sub64(t[42], mod[42], D)
	z[43], D = bits.Sub64(t[43], mod[43], D)
	z[44], D = bits.Sub64(t[44], mod[44], D)
	z[45], D = bits.Sub64(t[45], mod[45], D)
	z[46], D = bits.Sub64(t[46], mod[46], D)
	z[47], D = bits.Sub64(t[47], mod[47], D)
	z[48], D = bits.Sub64(t[48], mod[48], D)
	z[49], D = bits.Sub64(t[49], mod[49], D)
	z[50], D = bits.Sub64(t[50], mod[50], D)
	z[51], D = bits.Sub64(t[51], mod[51], D)
	z[52], D = bits.Sub64(t[52], mod[52], D)
	z[53], D = bits.Sub64(t[53], mod[53], D)
	z[54], D = bits.Sub64(t[54], mod[54], D)
	z[55], D = bits.Sub64(t[55], mod[55], D)
	z[56], D = bits.Sub64(t[56], mod[56], D)
	z[57], D = bits.Sub64(t[57], mod[57], D)
	z[58], D = bits.Sub64(t[58], mod[58], D)
	z[59], D = bits.Sub64(t[59], mod[59], D)

	if D != 0 && t[60] == 0 {
		// reduction was not necessary
		copy(z[:], t[:60])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled3904(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[61]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[61]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[61]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[61]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [62]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC1 = bits.Sub64(mod[42], x[42], gteC1)
	_, gteC1 = bits.Sub64(mod[43], x[43], gteC1)
	_, gteC1 = bits.Sub64(mod[44], x[44], gteC1)
	_, gteC1 = bits.Sub64(mod[45], x[45], gteC1)
	_, gteC1 = bits.Sub64(mod[46], x[46], gteC1)
	_, gteC1 = bits.Sub64(mod[47], x[47], gteC1)
	_, gteC1 = bits.Sub64(mod[48], x[48], gteC1)
	_, gteC1 = bits.Sub64(mod[49], x[49], gteC1)
	_, gteC1 = bits.Sub64(mod[50], x[50], gteC1)
	_, gteC1 = bits.Sub64(mod[51], x[51], gteC1)
	_, gteC1 = bits.Sub64(mod[52], x[52], gteC1)
	_, gteC1 = bits.Sub64(mod[53], x[53], gteC1)
	_, gteC1 = bits.Sub64(mod[54], x[54], gteC1)
	_, gteC1 = bits.Sub64(mod[55], x[55], gteC1)
	_, gteC1 = bits.Sub64(mod[56], x[56], gteC1)
	_, gteC1 = bits.Sub64(mod[57], x[57], gteC1)
	_, gteC1 = bits.Sub64(mod[58], x[58], gteC1)
	_, gteC1 = bits.Sub64(mod[59], x[59], gteC1)
	_, gteC1 = bits.Sub64(mod[60], x[60], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)
	_, gteC2 = bits.Sub64(mod[42], y[42], gteC2)
	_, gteC2 = bits.Sub64(mod[43], y[43], gteC2)
	_, gteC2 = bits.Sub64(mod[44], y[44], gteC2)
	_, gteC2 = bits.Sub64(mod[45], y[45], gteC2)
	_, gteC2 = bits.Sub64(mod[46], y[46], gteC2)
	_, gteC2 = bits.Sub64(mod[47], y[47], gteC2)
	_, gteC2 = bits.Sub64(mod[48], y[48], gteC2)
	_, gteC2 = bits.Sub64(mod[49], y[49], gteC2)
	_, gteC2 = bits.Sub64(mod[50], y[50], gteC2)
	_, gteC2 = bits.Sub64(mod[51], y[51], gteC2)
	_, gteC2 = bits.Sub64(mod[52], y[52], gteC2)
	_, gteC2 = bits.Sub64(mod[53], y[53], gteC2)
	_, gteC2 = bits.Sub64(mod[54], y[54], gteC2)
	_, gteC2 = bits.Sub64(mod[55], y[55], gteC2)
	_, gteC2 = bits.Sub64(mod[56], y[56], gteC2)
	_, gteC2 = bits.Sub64(mod[57], y[57], gteC2)
	_, gteC2 = bits.Sub64(mod[58], y[58], gteC2)
	_, gteC2 = bits.Sub64(mod[59], y[59], gteC2)
	_, gteC2 = bits.Sub64(mod[60], y[60], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)
	C, t[42] = madd1(x[0], y[42], C)
	C, t[43] = madd1(x[0], y[43], C)
	C, t[44] = madd1(x[0], y[44], C)
	C, t[45] = madd1(x[0], y[45], C)
	C, t[46] = madd1(x[0], y[46], C)
	C, t[47] = madd1(x[0], y[47], C)
	C, t[48] = madd1(x[0], y[48], C)
	C, t[49] = madd1(x[0], y[49], C)
	C, t[50] = madd1(x[0], y[50], C)
	C, t[51] = madd1(x[0], y[51], C)
	C, t[52] = madd1(x[0], y[52], C)
	C, t[53] = madd1(x[0], y[53], C)
	C, t[54] = madd1(x[0], y[54], C)
	C, t[55] = madd1(x[0], y[55], C)
	C, t[56] = madd1(x[0], y[56], C)
	C, t[57] = madd1(x[0], y[57], C)
	C, t[58] = madd1(x[0], y[58], C)
	C, t[59] = madd1(x[0], y[59], C)
	C, t[60] = madd1(x[0], y[60], C)

	t[61], D = bits.Add64(t[61], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	C, t[41] = madd2(m, mod[42], t[42], C)
	C, t[42] = madd2(m, mod[43], t[43], C)
	C, t[43] = madd2(m, mod[44], t[44], C)
	C, t[44] = madd2(m, mod[45], t[45], C)
	C, t[45] = madd2(m, mod[46], t[46], C)
	C, t[46] = madd2(m, mod[47], t[47], C)
	C, t[47] = madd2(m, mod[48], t[48], C)
	C, t[48] = madd2(m, mod[49], t[49], C)
	C, t[49] = madd2(m, mod[50], t[50], C)
	C, t[50] = madd2(m, mod[51], t[51], C)
	C, t[51] = madd2(m, mod[52], t[52], C)
	C, t[52] = madd2(m, mod[53], t[53], C)
	C, t[53] = madd2(m, mod[54], t[54], C)
	C, t[54] = madd2(m, mod[55], t[55], C)
	C, t[55] = madd2(m, mod[56], t[56], C)
	C, t[56] = madd2(m, mod[57], t[57], C)
	C, t[57] = madd2(m, mod[58], t[58], C)
	C, t[58] = madd2(m, mod[59], t[59], C)
	C, t[59] = madd2(m, mod[60], t[60], C)
	t[60], C = bits.Add64(t[61], C, 0)
	t[61], _ = bits.Add64(0, D, C)

	for j := 1; j < 61; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		C, t[42] = madd2(x[j], y[42], t[42], C)
		C, t[43] = madd2(x[j], y[43], t[43], C)
		C, t[44] = madd2(x[j], y[44], t[44], C)
		C, t[45] = madd2(x[j], y[45], t[45], C)
		C, t[46] = madd2(x[j], y[46], t[46], C)
		C, t[47] = madd2(x[j], y[47], t[47], C)
		C, t[48] = madd2(x[j], y[48], t[48], C)
		C, t[49] = madd2(x[j], y[49], t[49], C)
		C, t[50] = madd2(x[j], y[50], t[50], C)
		C, t[51] = madd2(x[j], y[51], t[51], C)
		C, t[52] = madd2(x[j], y[52], t[52], C)
		C, t[53] = madd2(x[j], y[53], t[53], C)
		C, t[54] = madd2(x[j], y[54], t[54], C)
		C, t[55] = madd2(x[j], y[55], t[55], C)
		C, t[56] = madd2(x[j], y[56], t[56], C)
		C, t[57] = madd2(x[j], y[57], t[57], C)
		C, t[58] = madd2(x[j], y[58], t[58], C)
		C, t[59] = madd2(x[j], y[59], t[59], C)
		C, t[60] = madd2(x[j], y[60], t[60], C)
		t[61], D = bits.Add64(t[61], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		C, t[41] = madd2(m, mod[42], t[42], C)
		C, t[42] = madd2(m, mod[43], t[43], C)
		C, t[43] = madd2(m, mod[44], t[44], C)
		C, t[44] = madd2(m, mod[45], t[45], C)
		C, t[45] = madd2(m, mod[46], t[46], C)
		C, t[46] = madd2(m, mod[47], t[47], C)
		C, t[47] = madd2(m, mod[48], t[48], C)
		C, t[48] = madd2(m, mod[49], t[49], C)
		C, t[49] = madd2(m, mod[50], t[50], C)
		C, t[50] = madd2(m, mod[51], t[51], C)
		C, t[51] = madd2(m, mod[52], t[52], C)
		C, t[52] = madd2(m, mod[53], t[53], C)
		C, t[53] = madd2(m, mod[54], t[54], C)
		C, t[54] = madd2(m, mod[55], t[55], C)
		C, t[55] = madd2(m, mod[56], t[56], C)
		C, t[56] = madd2(m, mod[57], t[57], C)
		C, t[57] = madd2(m, mod[58], t[58], C)
		C, t[58] = madd2(m, mod[59], t[59], C)
		C, t[59] = madd2(m, mod[60], t[60], C)
		t[60], C = bits.Add64(t[61], C, 0)
		t[61], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)
	z[42], D = bits.Sub64(t[42], mod[42], D)
	z[43], D = bits.Sub64(t[43], mod[43], D)
	z[44], D = bits.Sub64(t[44], mod[44], D)
	z[45], D = bits.Sub64(t[45], mod[45], D)
	z[46], D = bits.Sub64(t[46], mod[46], D)
	z[47], D = bits.Sub64(t[47], mod[47], D)
	z[48], D = bits.Sub64(t[48], mod[48], D)
	z[49], D = bits.Sub64(t[49], mod[49], D)
	z[50], D = bits.Sub64(t[50], mod[50], D)
	z[51], D = bits.Sub64(t[51], mod[51], D)
	z[52], D = bits.Sub64(t[52], mod[52], D)
	z[53], D = bits.Sub64(t[53], mod[53], D)
	z[54], D = bits.Sub64(t[54], mod[54], D)
	z[55], D = bits.Sub64(t[55], mod[55], D)
	z[56], D = bits.Sub64(t[56], mod[56], D)
	z[57], D = bits.Sub64(t[57], mod[57], D)
	z[58], D = bits.Sub64(t[58], mod[58], D)
	z[59], D = bits.Sub64(t[59], mod[59], D)
	z[60], D = bits.Sub64(t[60], mod[60], D)

	if D != 0 && t[61] == 0 {
		// reduction was not necessary
		copy(z[:], t[:61])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled3968(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[62]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[62]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[62]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[62]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [63]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC1 = bits.Sub64(mod[42], x[42], gteC1)
	_, gteC1 = bits.Sub64(mod[43], x[43], gteC1)
	_, gteC1 = bits.Sub64(mod[44], x[44], gteC1)
	_, gteC1 = bits.Sub64(mod[45], x[45], gteC1)
	_, gteC1 = bits.Sub64(mod[46], x[46], gteC1)
	_, gteC1 = bits.Sub64(mod[47], x[47], gteC1)
	_, gteC1 = bits.Sub64(mod[48], x[48], gteC1)
	_, gteC1 = bits.Sub64(mod[49], x[49], gteC1)
	_, gteC1 = bits.Sub64(mod[50], x[50], gteC1)
	_, gteC1 = bits.Sub64(mod[51], x[51], gteC1)
	_, gteC1 = bits.Sub64(mod[52], x[52], gteC1)
	_, gteC1 = bits.Sub64(mod[53], x[53], gteC1)
	_, gteC1 = bits.Sub64(mod[54], x[54], gteC1)
	_, gteC1 = bits.Sub64(mod[55], x[55], gteC1)
	_, gteC1 = bits.Sub64(mod[56], x[56], gteC1)
	_, gteC1 = bits.Sub64(mod[57], x[57], gteC1)
	_, gteC1 = bits.Sub64(mod[58], x[58], gteC1)
	_, gteC1 = bits.Sub64(mod[59], x[59], gteC1)
	_, gteC1 = bits.Sub64(mod[60], x[60], gteC1)
	_, gteC1 = bits.Sub64(mod[61], x[61], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)
	_, gteC2 = bits.Sub64(mod[42], y[42], gteC2)
	_, gteC2 = bits.Sub64(mod[43], y[43], gteC2)
	_, gteC2 = bits.Sub64(mod[44], y[44], gteC2)
	_, gteC2 = bits.Sub64(mod[45], y[45], gteC2)
	_, gteC2 = bits.Sub64(mod[46], y[46], gteC2)
	_, gteC2 = bits.Sub64(mod[47], y[47], gteC2)
	_, gteC2 = bits.Sub64(mod[48], y[48], gteC2)
	_, gteC2 = bits.Sub64(mod[49], y[49], gteC2)
	_, gteC2 = bits.Sub64(mod[50], y[50], gteC2)
	_, gteC2 = bits.Sub64(mod[51], y[51], gteC2)
	_, gteC2 = bits.Sub64(mod[52], y[52], gteC2)
	_, gteC2 = bits.Sub64(mod[53], y[53], gteC2)
	_, gteC2 = bits.Sub64(mod[54], y[54], gteC2)
	_, gteC2 = bits.Sub64(mod[55], y[55], gteC2)
	_, gteC2 = bits.Sub64(mod[56], y[56], gteC2)
	_, gteC2 = bits.Sub64(mod[57], y[57], gteC2)
	_, gteC2 = bits.Sub64(mod[58], y[58], gteC2)
	_, gteC2 = bits.Sub64(mod[59], y[59], gteC2)
	_, gteC2 = bits.Sub64(mod[60], y[60], gteC2)
	_, gteC2 = bits.Sub64(mod[61], y[61], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)
	C, t[42] = madd1(x[0], y[42], C)
	C, t[43] = madd1(x[0], y[43], C)
	C, t[44] = madd1(x[0], y[44], C)
	C, t[45] = madd1(x[0], y[45], C)
	C, t[46] = madd1(x[0], y[46], C)
	C, t[47] = madd1(x[0], y[47], C)
	C, t[48] = madd1(x[0], y[48], C)
	C, t[49] = madd1(x[0], y[49], C)
	C, t[50] = madd1(x[0], y[50], C)
	C, t[51] = madd1(x[0], y[51], C)
	C, t[52] = madd1(x[0], y[52], C)
	C, t[53] = madd1(x[0], y[53], C)
	C, t[54] = madd1(x[0], y[54], C)
	C, t[55] = madd1(x[0], y[55], C)
	C, t[56] = madd1(x[0], y[56], C)
	C, t[57] = madd1(x[0], y[57], C)
	C, t[58] = madd1(x[0], y[58], C)
	C, t[59] = madd1(x[0], y[59], C)
	C, t[60] = madd1(x[0], y[60], C)
	C, t[61] = madd1(x[0], y[61], C)

	t[62], D = bits.Add64(t[62], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	C, t[41] = madd2(m, mod[42], t[42], C)
	C, t[42] = madd2(m, mod[43], t[43], C)
	C, t[43] = madd2(m, mod[44], t[44], C)
	C, t[44] = madd2(m, mod[45], t[45], C)
	C, t[45] = madd2(m, mod[46], t[46], C)
	C, t[46] = madd2(m, mod[47], t[47], C)
	C, t[47] = madd2(m, mod[48], t[48], C)
	C, t[48] = madd2(m, mod[49], t[49], C)
	C, t[49] = madd2(m, mod[50], t[50], C)
	C, t[50] = madd2(m, mod[51], t[51], C)
	C, t[51] = madd2(m, mod[52], t[52], C)
	C, t[52] = madd2(m, mod[53], t[53], C)
	C, t[53] = madd2(m, mod[54], t[54], C)
	C, t[54] = madd2(m, mod[55], t[55], C)
	C, t[55] = madd2(m, mod[56], t[56], C)
	C, t[56] = madd2(m, mod[57], t[57], C)
	C, t[57] = madd2(m, mod[58], t[58], C)
	C, t[58] = madd2(m, mod[59], t[59], C)
	C, t[59] = madd2(m, mod[60], t[60], C)
	C, t[60] = madd2(m, mod[61], t[61], C)
	t[61], C = bits.Add64(t[62], C, 0)
	t[62], _ = bits.Add64(0, D, C)

	for j := 1; j < 62; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		C, t[42] = madd2(x[j], y[42], t[42], C)
		C, t[43] = madd2(x[j], y[43], t[43], C)
		C, t[44] = madd2(x[j], y[44], t[44], C)
		C, t[45] = madd2(x[j], y[45], t[45], C)
		C, t[46] = madd2(x[j], y[46], t[46], C)
		C, t[47] = madd2(x[j], y[47], t[47], C)
		C, t[48] = madd2(x[j], y[48], t[48], C)
		C, t[49] = madd2(x[j], y[49], t[49], C)
		C, t[50] = madd2(x[j], y[50], t[50], C)
		C, t[51] = madd2(x[j], y[51], t[51], C)
		C, t[52] = madd2(x[j], y[52], t[52], C)
		C, t[53] = madd2(x[j], y[53], t[53], C)
		C, t[54] = madd2(x[j], y[54], t[54], C)
		C, t[55] = madd2(x[j], y[55], t[55], C)
		C, t[56] = madd2(x[j], y[56], t[56], C)
		C, t[57] = madd2(x[j], y[57], t[57], C)
		C, t[58] = madd2(x[j], y[58], t[58], C)
		C, t[59] = madd2(x[j], y[59], t[59], C)
		C, t[60] = madd2(x[j], y[60], t[60], C)
		C, t[61] = madd2(x[j], y[61], t[61], C)
		t[62], D = bits.Add64(t[62], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		C, t[41] = madd2(m, mod[42], t[42], C)
		C, t[42] = madd2(m, mod[43], t[43], C)
		C, t[43] = madd2(m, mod[44], t[44], C)
		C, t[44] = madd2(m, mod[45], t[45], C)
		C, t[45] = madd2(m, mod[46], t[46], C)
		C, t[46] = madd2(m, mod[47], t[47], C)
		C, t[47] = madd2(m, mod[48], t[48], C)
		C, t[48] = madd2(m, mod[49], t[49], C)
		C, t[49] = madd2(m, mod[50], t[50], C)
		C, t[50] = madd2(m, mod[51], t[51], C)
		C, t[51] = madd2(m, mod[52], t[52], C)
		C, t[52] = madd2(m, mod[53], t[53], C)
		C, t[53] = madd2(m, mod[54], t[54], C)
		C, t[54] = madd2(m, mod[55], t[55], C)
		C, t[55] = madd2(m, mod[56], t[56], C)
		C, t[56] = madd2(m, mod[57], t[57], C)
		C, t[57] = madd2(m, mod[58], t[58], C)
		C, t[58] = madd2(m, mod[59], t[59], C)
		C, t[59] = madd2(m, mod[60], t[60], C)
		C, t[60] = madd2(m, mod[61], t[61], C)
		t[61], C = bits.Add64(t[62], C, 0)
		t[62], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)
	z[42], D = bits.Sub64(t[42], mod[42], D)
	z[43], D = bits.Sub64(t[43], mod[43], D)
	z[44], D = bits.Sub64(t[44], mod[44], D)
	z[45], D = bits.Sub64(t[45], mod[45], D)
	z[46], D = bits.Sub64(t[46], mod[46], D)
	z[47], D = bits.Sub64(t[47], mod[47], D)
	z[48], D = bits.Sub64(t[48], mod[48], D)
	z[49], D = bits.Sub64(t[49], mod[49], D)
	z[50], D = bits.Sub64(t[50], mod[50], D)
	z[51], D = bits.Sub64(t[51], mod[51], D)
	z[52], D = bits.Sub64(t[52], mod[52], D)
	z[53], D = bits.Sub64(t[53], mod[53], D)
	z[54], D = bits.Sub64(t[54], mod[54], D)
	z[55], D = bits.Sub64(t[55], mod[55], D)
	z[56], D = bits.Sub64(t[56], mod[56], D)
	z[57], D = bits.Sub64(t[57], mod[57], D)
	z[58], D = bits.Sub64(t[58], mod[58], D)
	z[59], D = bits.Sub64(t[59], mod[59], D)
	z[60], D = bits.Sub64(t[60], mod[60], D)
	z[61], D = bits.Sub64(t[61], mod[61], D)

	if D != 0 && t[62] == 0 {
		// reduction was not necessary
		copy(z[:], t[:62])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled4032(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[63]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[63]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[63]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[63]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [64]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC1 = bits.Sub64(mod[42], x[42], gteC1)
	_, gteC1 = bits.Sub64(mod[43], x[43], gteC1)
	_, gteC1 = bits.Sub64(mod[44], x[44], gteC1)
	_, gteC1 = bits.Sub64(mod[45], x[45], gteC1)
	_, gteC1 = bits.Sub64(mod[46], x[46], gteC1)
	_, gteC1 = bits.Sub64(mod[47], x[47], gteC1)
	_, gteC1 = bits.Sub64(mod[48], x[48], gteC1)
	_, gteC1 = bits.Sub64(mod[49], x[49], gteC1)
	_, gteC1 = bits.Sub64(mod[50], x[50], gteC1)
	_, gteC1 = bits.Sub64(mod[51], x[51], gteC1)
	_, gteC1 = bits.Sub64(mod[52], x[52], gteC1)
	_, gteC1 = bits.Sub64(mod[53], x[53], gteC1)
	_, gteC1 = bits.Sub64(mod[54], x[54], gteC1)
	_, gteC1 = bits.Sub64(mod[55], x[55], gteC1)
	_, gteC1 = bits.Sub64(mod[56], x[56], gteC1)
	_, gteC1 = bits.Sub64(mod[57], x[57], gteC1)
	_, gteC1 = bits.Sub64(mod[58], x[58], gteC1)
	_, gteC1 = bits.Sub64(mod[59], x[59], gteC1)
	_, gteC1 = bits.Sub64(mod[60], x[60], gteC1)
	_, gteC1 = bits.Sub64(mod[61], x[61], gteC1)
	_, gteC1 = bits.Sub64(mod[62], x[62], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)
	_, gteC2 = bits.Sub64(mod[42], y[42], gteC2)
	_, gteC2 = bits.Sub64(mod[43], y[43], gteC2)
	_, gteC2 = bits.Sub64(mod[44], y[44], gteC2)
	_, gteC2 = bits.Sub64(mod[45], y[45], gteC2)
	_, gteC2 = bits.Sub64(mod[46], y[46], gteC2)
	_, gteC2 = bits.Sub64(mod[47], y[47], gteC2)
	_, gteC2 = bits.Sub64(mod[48], y[48], gteC2)
	_, gteC2 = bits.Sub64(mod[49], y[49], gteC2)
	_, gteC2 = bits.Sub64(mod[50], y[50], gteC2)
	_, gteC2 = bits.Sub64(mod[51], y[51], gteC2)
	_, gteC2 = bits.Sub64(mod[52], y[52], gteC2)
	_, gteC2 = bits.Sub64(mod[53], y[53], gteC2)
	_, gteC2 = bits.Sub64(mod[54], y[54], gteC2)
	_, gteC2 = bits.Sub64(mod[55], y[55], gteC2)
	_, gteC2 = bits.Sub64(mod[56], y[56], gteC2)
	_, gteC2 = bits.Sub64(mod[57], y[57], gteC2)
	_, gteC2 = bits.Sub64(mod[58], y[58], gteC2)
	_, gteC2 = bits.Sub64(mod[59], y[59], gteC2)
	_, gteC2 = bits.Sub64(mod[60], y[60], gteC2)
	_, gteC2 = bits.Sub64(mod[61], y[61], gteC2)
	_, gteC2 = bits.Sub64(mod[62], y[62], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)
	C, t[42] = madd1(x[0], y[42], C)
	C, t[43] = madd1(x[0], y[43], C)
	C, t[44] = madd1(x[0], y[44], C)
	C, t[45] = madd1(x[0], y[45], C)
	C, t[46] = madd1(x[0], y[46], C)
	C, t[47] = madd1(x[0], y[47], C)
	C, t[48] = madd1(x[0], y[48], C)
	C, t[49] = madd1(x[0], y[49], C)
	C, t[50] = madd1(x[0], y[50], C)
	C, t[51] = madd1(x[0], y[51], C)
	C, t[52] = madd1(x[0], y[52], C)
	C, t[53] = madd1(x[0], y[53], C)
	C, t[54] = madd1(x[0], y[54], C)
	C, t[55] = madd1(x[0], y[55], C)
	C, t[56] = madd1(x[0], y[56], C)
	C, t[57] = madd1(x[0], y[57], C)
	C, t[58] = madd1(x[0], y[58], C)
	C, t[59] = madd1(x[0], y[59], C)
	C, t[60] = madd1(x[0], y[60], C)
	C, t[61] = madd1(x[0], y[61], C)
	C, t[62] = madd1(x[0], y[62], C)

	t[63], D = bits.Add64(t[63], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	C, t[41] = madd2(m, mod[42], t[42], C)
	C, t[42] = madd2(m, mod[43], t[43], C)
	C, t[43] = madd2(m, mod[44], t[44], C)
	C, t[44] = madd2(m, mod[45], t[45], C)
	C, t[45] = madd2(m, mod[46], t[46], C)
	C, t[46] = madd2(m, mod[47], t[47], C)
	C, t[47] = madd2(m, mod[48], t[48], C)
	C, t[48] = madd2(m, mod[49], t[49], C)
	C, t[49] = madd2(m, mod[50], t[50], C)
	C, t[50] = madd2(m, mod[51], t[51], C)
	C, t[51] = madd2(m, mod[52], t[52], C)
	C, t[52] = madd2(m, mod[53], t[53], C)
	C, t[53] = madd2(m, mod[54], t[54], C)
	C, t[54] = madd2(m, mod[55], t[55], C)
	C, t[55] = madd2(m, mod[56], t[56], C)
	C, t[56] = madd2(m, mod[57], t[57], C)
	C, t[57] = madd2(m, mod[58], t[58], C)
	C, t[58] = madd2(m, mod[59], t[59], C)
	C, t[59] = madd2(m, mod[60], t[60], C)
	C, t[60] = madd2(m, mod[61], t[61], C)
	C, t[61] = madd2(m, mod[62], t[62], C)
	t[62], C = bits.Add64(t[63], C, 0)
	t[63], _ = bits.Add64(0, D, C)

	for j := 1; j < 63; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		C, t[42] = madd2(x[j], y[42], t[42], C)
		C, t[43] = madd2(x[j], y[43], t[43], C)
		C, t[44] = madd2(x[j], y[44], t[44], C)
		C, t[45] = madd2(x[j], y[45], t[45], C)
		C, t[46] = madd2(x[j], y[46], t[46], C)
		C, t[47] = madd2(x[j], y[47], t[47], C)
		C, t[48] = madd2(x[j], y[48], t[48], C)
		C, t[49] = madd2(x[j], y[49], t[49], C)
		C, t[50] = madd2(x[j], y[50], t[50], C)
		C, t[51] = madd2(x[j], y[51], t[51], C)
		C, t[52] = madd2(x[j], y[52], t[52], C)
		C, t[53] = madd2(x[j], y[53], t[53], C)
		C, t[54] = madd2(x[j], y[54], t[54], C)
		C, t[55] = madd2(x[j], y[55], t[55], C)
		C, t[56] = madd2(x[j], y[56], t[56], C)
		C, t[57] = madd2(x[j], y[57], t[57], C)
		C, t[58] = madd2(x[j], y[58], t[58], C)
		C, t[59] = madd2(x[j], y[59], t[59], C)
		C, t[60] = madd2(x[j], y[60], t[60], C)
		C, t[61] = madd2(x[j], y[61], t[61], C)
		C, t[62] = madd2(x[j], y[62], t[62], C)
		t[63], D = bits.Add64(t[63], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		C, t[41] = madd2(m, mod[42], t[42], C)
		C, t[42] = madd2(m, mod[43], t[43], C)
		C, t[43] = madd2(m, mod[44], t[44], C)
		C, t[44] = madd2(m, mod[45], t[45], C)
		C, t[45] = madd2(m, mod[46], t[46], C)
		C, t[46] = madd2(m, mod[47], t[47], C)
		C, t[47] = madd2(m, mod[48], t[48], C)
		C, t[48] = madd2(m, mod[49], t[49], C)
		C, t[49] = madd2(m, mod[50], t[50], C)
		C, t[50] = madd2(m, mod[51], t[51], C)
		C, t[51] = madd2(m, mod[52], t[52], C)
		C, t[52] = madd2(m, mod[53], t[53], C)
		C, t[53] = madd2(m, mod[54], t[54], C)
		C, t[54] = madd2(m, mod[55], t[55], C)
		C, t[55] = madd2(m, mod[56], t[56], C)
		C, t[56] = madd2(m, mod[57], t[57], C)
		C, t[57] = madd2(m, mod[58], t[58], C)
		C, t[58] = madd2(m, mod[59], t[59], C)
		C, t[59] = madd2(m, mod[60], t[60], C)
		C, t[60] = madd2(m, mod[61], t[61], C)
		C, t[61] = madd2(m, mod[62], t[62], C)
		t[62], C = bits.Add64(t[63], C, 0)
		t[63], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)
	z[42], D = bits.Sub64(t[42], mod[42], D)
	z[43], D = bits.Sub64(t[43], mod[43], D)
	z[44], D = bits.Sub64(t[44], mod[44], D)
	z[45], D = bits.Sub64(t[45], mod[45], D)
	z[46], D = bits.Sub64(t[46], mod[46], D)
	z[47], D = bits.Sub64(t[47], mod[47], D)
	z[48], D = bits.Sub64(t[48], mod[48], D)
	z[49], D = bits.Sub64(t[49], mod[49], D)
	z[50], D = bits.Sub64(t[50], mod[50], D)
	z[51], D = bits.Sub64(t[51], mod[51], D)
	z[52], D = bits.Sub64(t[52], mod[52], D)
	z[53], D = bits.Sub64(t[53], mod[53], D)
	z[54], D = bits.Sub64(t[54], mod[54], D)
	z[55], D = bits.Sub64(t[55], mod[55], D)
	z[56], D = bits.Sub64(t[56], mod[56], D)
	z[57], D = bits.Sub64(t[57], mod[57], D)
	z[58], D = bits.Sub64(t[58], mod[58], D)
	z[59], D = bits.Sub64(t[59], mod[59], D)
	z[60], D = bits.Sub64(t[60], mod[60], D)
	z[61], D = bits.Sub64(t[61], mod[61], D)
	z[62], D = bits.Sub64(t[62], mod[62], D)

	if D != 0 && t[63] == 0 {
		// reduction was not necessary
		copy(z[:], t[:63])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}

func MulMontNonUnrolled4096(ctx *Field, out_bytes, x_bytes, y_bytes []byte) error {
	x := (*[64]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[64]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[64]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[64]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [65]uint64
	var D uint64
	var m, C uint64

	var gteC1, gteC2 uint64
	_, gteC1 = bits.Sub64(mod[0], x[0], gteC1)
	_, gteC1 = bits.Sub64(mod[1], x[1], gteC1)
	_, gteC1 = bits.Sub64(mod[2], x[2], gteC1)
	_, gteC1 = bits.Sub64(mod[3], x[3], gteC1)
	_, gteC1 = bits.Sub64(mod[4], x[4], gteC1)
	_, gteC1 = bits.Sub64(mod[5], x[5], gteC1)
	_, gteC1 = bits.Sub64(mod[6], x[6], gteC1)
	_, gteC1 = bits.Sub64(mod[7], x[7], gteC1)
	_, gteC1 = bits.Sub64(mod[8], x[8], gteC1)
	_, gteC1 = bits.Sub64(mod[9], x[9], gteC1)
	_, gteC1 = bits.Sub64(mod[10], x[10], gteC1)
	_, gteC1 = bits.Sub64(mod[11], x[11], gteC1)
	_, gteC1 = bits.Sub64(mod[12], x[12], gteC1)
	_, gteC1 = bits.Sub64(mod[13], x[13], gteC1)
	_, gteC1 = bits.Sub64(mod[14], x[14], gteC1)
	_, gteC1 = bits.Sub64(mod[15], x[15], gteC1)
	_, gteC1 = bits.Sub64(mod[16], x[16], gteC1)
	_, gteC1 = bits.Sub64(mod[17], x[17], gteC1)
	_, gteC1 = bits.Sub64(mod[18], x[18], gteC1)
	_, gteC1 = bits.Sub64(mod[19], x[19], gteC1)
	_, gteC1 = bits.Sub64(mod[20], x[20], gteC1)
	_, gteC1 = bits.Sub64(mod[21], x[21], gteC1)
	_, gteC1 = bits.Sub64(mod[22], x[22], gteC1)
	_, gteC1 = bits.Sub64(mod[23], x[23], gteC1)
	_, gteC1 = bits.Sub64(mod[24], x[24], gteC1)
	_, gteC1 = bits.Sub64(mod[25], x[25], gteC1)
	_, gteC1 = bits.Sub64(mod[26], x[26], gteC1)
	_, gteC1 = bits.Sub64(mod[27], x[27], gteC1)
	_, gteC1 = bits.Sub64(mod[28], x[28], gteC1)
	_, gteC1 = bits.Sub64(mod[29], x[29], gteC1)
	_, gteC1 = bits.Sub64(mod[30], x[30], gteC1)
	_, gteC1 = bits.Sub64(mod[31], x[31], gteC1)
	_, gteC1 = bits.Sub64(mod[32], x[32], gteC1)
	_, gteC1 = bits.Sub64(mod[33], x[33], gteC1)
	_, gteC1 = bits.Sub64(mod[34], x[34], gteC1)
	_, gteC1 = bits.Sub64(mod[35], x[35], gteC1)
	_, gteC1 = bits.Sub64(mod[36], x[36], gteC1)
	_, gteC1 = bits.Sub64(mod[37], x[37], gteC1)
	_, gteC1 = bits.Sub64(mod[38], x[38], gteC1)
	_, gteC1 = bits.Sub64(mod[39], x[39], gteC1)
	_, gteC1 = bits.Sub64(mod[40], x[40], gteC1)
	_, gteC1 = bits.Sub64(mod[41], x[41], gteC1)
	_, gteC1 = bits.Sub64(mod[42], x[42], gteC1)
	_, gteC1 = bits.Sub64(mod[43], x[43], gteC1)
	_, gteC1 = bits.Sub64(mod[44], x[44], gteC1)
	_, gteC1 = bits.Sub64(mod[45], x[45], gteC1)
	_, gteC1 = bits.Sub64(mod[46], x[46], gteC1)
	_, gteC1 = bits.Sub64(mod[47], x[47], gteC1)
	_, gteC1 = bits.Sub64(mod[48], x[48], gteC1)
	_, gteC1 = bits.Sub64(mod[49], x[49], gteC1)
	_, gteC1 = bits.Sub64(mod[50], x[50], gteC1)
	_, gteC1 = bits.Sub64(mod[51], x[51], gteC1)
	_, gteC1 = bits.Sub64(mod[52], x[52], gteC1)
	_, gteC1 = bits.Sub64(mod[53], x[53], gteC1)
	_, gteC1 = bits.Sub64(mod[54], x[54], gteC1)
	_, gteC1 = bits.Sub64(mod[55], x[55], gteC1)
	_, gteC1 = bits.Sub64(mod[56], x[56], gteC1)
	_, gteC1 = bits.Sub64(mod[57], x[57], gteC1)
	_, gteC1 = bits.Sub64(mod[58], x[58], gteC1)
	_, gteC1 = bits.Sub64(mod[59], x[59], gteC1)
	_, gteC1 = bits.Sub64(mod[60], x[60], gteC1)
	_, gteC1 = bits.Sub64(mod[61], x[61], gteC1)
	_, gteC1 = bits.Sub64(mod[62], x[62], gteC1)
	_, gteC1 = bits.Sub64(mod[63], x[63], gteC1)
	_, gteC2 = bits.Sub64(mod[0], y[0], gteC2)
	_, gteC2 = bits.Sub64(mod[1], y[1], gteC2)
	_, gteC2 = bits.Sub64(mod[2], y[2], gteC2)
	_, gteC2 = bits.Sub64(mod[3], y[3], gteC2)
	_, gteC2 = bits.Sub64(mod[4], y[4], gteC2)
	_, gteC2 = bits.Sub64(mod[5], y[5], gteC2)
	_, gteC2 = bits.Sub64(mod[6], y[6], gteC2)
	_, gteC2 = bits.Sub64(mod[7], y[7], gteC2)
	_, gteC2 = bits.Sub64(mod[8], y[8], gteC2)
	_, gteC2 = bits.Sub64(mod[9], y[9], gteC2)
	_, gteC2 = bits.Sub64(mod[10], y[10], gteC2)
	_, gteC2 = bits.Sub64(mod[11], y[11], gteC2)
	_, gteC2 = bits.Sub64(mod[12], y[12], gteC2)
	_, gteC2 = bits.Sub64(mod[13], y[13], gteC2)
	_, gteC2 = bits.Sub64(mod[14], y[14], gteC2)
	_, gteC2 = bits.Sub64(mod[15], y[15], gteC2)
	_, gteC2 = bits.Sub64(mod[16], y[16], gteC2)
	_, gteC2 = bits.Sub64(mod[17], y[17], gteC2)
	_, gteC2 = bits.Sub64(mod[18], y[18], gteC2)
	_, gteC2 = bits.Sub64(mod[19], y[19], gteC2)
	_, gteC2 = bits.Sub64(mod[20], y[20], gteC2)
	_, gteC2 = bits.Sub64(mod[21], y[21], gteC2)
	_, gteC2 = bits.Sub64(mod[22], y[22], gteC2)
	_, gteC2 = bits.Sub64(mod[23], y[23], gteC2)
	_, gteC2 = bits.Sub64(mod[24], y[24], gteC2)
	_, gteC2 = bits.Sub64(mod[25], y[25], gteC2)
	_, gteC2 = bits.Sub64(mod[26], y[26], gteC2)
	_, gteC2 = bits.Sub64(mod[27], y[27], gteC2)
	_, gteC2 = bits.Sub64(mod[28], y[28], gteC2)
	_, gteC2 = bits.Sub64(mod[29], y[29], gteC2)
	_, gteC2 = bits.Sub64(mod[30], y[30], gteC2)
	_, gteC2 = bits.Sub64(mod[31], y[31], gteC2)
	_, gteC2 = bits.Sub64(mod[32], y[32], gteC2)
	_, gteC2 = bits.Sub64(mod[33], y[33], gteC2)
	_, gteC2 = bits.Sub64(mod[34], y[34], gteC2)
	_, gteC2 = bits.Sub64(mod[35], y[35], gteC2)
	_, gteC2 = bits.Sub64(mod[36], y[36], gteC2)
	_, gteC2 = bits.Sub64(mod[37], y[37], gteC2)
	_, gteC2 = bits.Sub64(mod[38], y[38], gteC2)
	_, gteC2 = bits.Sub64(mod[39], y[39], gteC2)
	_, gteC2 = bits.Sub64(mod[40], y[40], gteC2)
	_, gteC2 = bits.Sub64(mod[41], y[41], gteC2)
	_, gteC2 = bits.Sub64(mod[42], y[42], gteC2)
	_, gteC2 = bits.Sub64(mod[43], y[43], gteC2)
	_, gteC2 = bits.Sub64(mod[44], y[44], gteC2)
	_, gteC2 = bits.Sub64(mod[45], y[45], gteC2)
	_, gteC2 = bits.Sub64(mod[46], y[46], gteC2)
	_, gteC2 = bits.Sub64(mod[47], y[47], gteC2)
	_, gteC2 = bits.Sub64(mod[48], y[48], gteC2)
	_, gteC2 = bits.Sub64(mod[49], y[49], gteC2)
	_, gteC2 = bits.Sub64(mod[50], y[50], gteC2)
	_, gteC2 = bits.Sub64(mod[51], y[51], gteC2)
	_, gteC2 = bits.Sub64(mod[52], y[52], gteC2)
	_, gteC2 = bits.Sub64(mod[53], y[53], gteC2)
	_, gteC2 = bits.Sub64(mod[54], y[54], gteC2)
	_, gteC2 = bits.Sub64(mod[55], y[55], gteC2)
	_, gteC2 = bits.Sub64(mod[56], y[56], gteC2)
	_, gteC2 = bits.Sub64(mod[57], y[57], gteC2)
	_, gteC2 = bits.Sub64(mod[58], y[58], gteC2)
	_, gteC2 = bits.Sub64(mod[59], y[59], gteC2)
	_, gteC2 = bits.Sub64(mod[60], y[60], gteC2)
	_, gteC2 = bits.Sub64(mod[61], y[61], gteC2)
	_, gteC2 = bits.Sub64(mod[62], y[62], gteC2)
	_, gteC2 = bits.Sub64(mod[63], y[63], gteC2)

	if gteC1 != 0 || gteC2 != 0 {
		return errors.New(fmt.Sprintf("input gte modulus"))
	}

	C, t[0] = bits.Mul64(x[0], y[0])
	C, t[1] = madd1(x[0], y[1], C)
	C, t[2] = madd1(x[0], y[2], C)
	C, t[3] = madd1(x[0], y[3], C)
	C, t[4] = madd1(x[0], y[4], C)
	C, t[5] = madd1(x[0], y[5], C)
	C, t[6] = madd1(x[0], y[6], C)
	C, t[7] = madd1(x[0], y[7], C)
	C, t[8] = madd1(x[0], y[8], C)
	C, t[9] = madd1(x[0], y[9], C)
	C, t[10] = madd1(x[0], y[10], C)
	C, t[11] = madd1(x[0], y[11], C)
	C, t[12] = madd1(x[0], y[12], C)
	C, t[13] = madd1(x[0], y[13], C)
	C, t[14] = madd1(x[0], y[14], C)
	C, t[15] = madd1(x[0], y[15], C)
	C, t[16] = madd1(x[0], y[16], C)
	C, t[17] = madd1(x[0], y[17], C)
	C, t[18] = madd1(x[0], y[18], C)
	C, t[19] = madd1(x[0], y[19], C)
	C, t[20] = madd1(x[0], y[20], C)
	C, t[21] = madd1(x[0], y[21], C)
	C, t[22] = madd1(x[0], y[22], C)
	C, t[23] = madd1(x[0], y[23], C)
	C, t[24] = madd1(x[0], y[24], C)
	C, t[25] = madd1(x[0], y[25], C)
	C, t[26] = madd1(x[0], y[26], C)
	C, t[27] = madd1(x[0], y[27], C)
	C, t[28] = madd1(x[0], y[28], C)
	C, t[29] = madd1(x[0], y[29], C)
	C, t[30] = madd1(x[0], y[30], C)
	C, t[31] = madd1(x[0], y[31], C)
	C, t[32] = madd1(x[0], y[32], C)
	C, t[33] = madd1(x[0], y[33], C)
	C, t[34] = madd1(x[0], y[34], C)
	C, t[35] = madd1(x[0], y[35], C)
	C, t[36] = madd1(x[0], y[36], C)
	C, t[37] = madd1(x[0], y[37], C)
	C, t[38] = madd1(x[0], y[38], C)
	C, t[39] = madd1(x[0], y[39], C)
	C, t[40] = madd1(x[0], y[40], C)
	C, t[41] = madd1(x[0], y[41], C)
	C, t[42] = madd1(x[0], y[42], C)
	C, t[43] = madd1(x[0], y[43], C)
	C, t[44] = madd1(x[0], y[44], C)
	C, t[45] = madd1(x[0], y[45], C)
	C, t[46] = madd1(x[0], y[46], C)
	C, t[47] = madd1(x[0], y[47], C)
	C, t[48] = madd1(x[0], y[48], C)
	C, t[49] = madd1(x[0], y[49], C)
	C, t[50] = madd1(x[0], y[50], C)
	C, t[51] = madd1(x[0], y[51], C)
	C, t[52] = madd1(x[0], y[52], C)
	C, t[53] = madd1(x[0], y[53], C)
	C, t[54] = madd1(x[0], y[54], C)
	C, t[55] = madd1(x[0], y[55], C)
	C, t[56] = madd1(x[0], y[56], C)
	C, t[57] = madd1(x[0], y[57], C)
	C, t[58] = madd1(x[0], y[58], C)
	C, t[59] = madd1(x[0], y[59], C)
	C, t[60] = madd1(x[0], y[60], C)
	C, t[61] = madd1(x[0], y[61], C)
	C, t[62] = madd1(x[0], y[62], C)
	C, t[63] = madd1(x[0], y[63], C)

	t[64], D = bits.Add64(t[64], C, 0)
	// m = t[0]n'[0] mod W
	m = t[0] * ctx.MontParamInterleaved

	// -----------------------------------
	// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
	C = madd0(m, mod[0], t[0])
	C, t[0] = madd2(m, mod[1], t[1], C)
	C, t[1] = madd2(m, mod[2], t[2], C)
	C, t[2] = madd2(m, mod[3], t[3], C)
	C, t[3] = madd2(m, mod[4], t[4], C)
	C, t[4] = madd2(m, mod[5], t[5], C)
	C, t[5] = madd2(m, mod[6], t[6], C)
	C, t[6] = madd2(m, mod[7], t[7], C)
	C, t[7] = madd2(m, mod[8], t[8], C)
	C, t[8] = madd2(m, mod[9], t[9], C)
	C, t[9] = madd2(m, mod[10], t[10], C)
	C, t[10] = madd2(m, mod[11], t[11], C)
	C, t[11] = madd2(m, mod[12], t[12], C)
	C, t[12] = madd2(m, mod[13], t[13], C)
	C, t[13] = madd2(m, mod[14], t[14], C)
	C, t[14] = madd2(m, mod[15], t[15], C)
	C, t[15] = madd2(m, mod[16], t[16], C)
	C, t[16] = madd2(m, mod[17], t[17], C)
	C, t[17] = madd2(m, mod[18], t[18], C)
	C, t[18] = madd2(m, mod[19], t[19], C)
	C, t[19] = madd2(m, mod[20], t[20], C)
	C, t[20] = madd2(m, mod[21], t[21], C)
	C, t[21] = madd2(m, mod[22], t[22], C)
	C, t[22] = madd2(m, mod[23], t[23], C)
	C, t[23] = madd2(m, mod[24], t[24], C)
	C, t[24] = madd2(m, mod[25], t[25], C)
	C, t[25] = madd2(m, mod[26], t[26], C)
	C, t[26] = madd2(m, mod[27], t[27], C)
	C, t[27] = madd2(m, mod[28], t[28], C)
	C, t[28] = madd2(m, mod[29], t[29], C)
	C, t[29] = madd2(m, mod[30], t[30], C)
	C, t[30] = madd2(m, mod[31], t[31], C)
	C, t[31] = madd2(m, mod[32], t[32], C)
	C, t[32] = madd2(m, mod[33], t[33], C)
	C, t[33] = madd2(m, mod[34], t[34], C)
	C, t[34] = madd2(m, mod[35], t[35], C)
	C, t[35] = madd2(m, mod[36], t[36], C)
	C, t[36] = madd2(m, mod[37], t[37], C)
	C, t[37] = madd2(m, mod[38], t[38], C)
	C, t[38] = madd2(m, mod[39], t[39], C)
	C, t[39] = madd2(m, mod[40], t[40], C)
	C, t[40] = madd2(m, mod[41], t[41], C)
	C, t[41] = madd2(m, mod[42], t[42], C)
	C, t[42] = madd2(m, mod[43], t[43], C)
	C, t[43] = madd2(m, mod[44], t[44], C)
	C, t[44] = madd2(m, mod[45], t[45], C)
	C, t[45] = madd2(m, mod[46], t[46], C)
	C, t[46] = madd2(m, mod[47], t[47], C)
	C, t[47] = madd2(m, mod[48], t[48], C)
	C, t[48] = madd2(m, mod[49], t[49], C)
	C, t[49] = madd2(m, mod[50], t[50], C)
	C, t[50] = madd2(m, mod[51], t[51], C)
	C, t[51] = madd2(m, mod[52], t[52], C)
	C, t[52] = madd2(m, mod[53], t[53], C)
	C, t[53] = madd2(m, mod[54], t[54], C)
	C, t[54] = madd2(m, mod[55], t[55], C)
	C, t[55] = madd2(m, mod[56], t[56], C)
	C, t[56] = madd2(m, mod[57], t[57], C)
	C, t[57] = madd2(m, mod[58], t[58], C)
	C, t[58] = madd2(m, mod[59], t[59], C)
	C, t[59] = madd2(m, mod[60], t[60], C)
	C, t[60] = madd2(m, mod[61], t[61], C)
	C, t[61] = madd2(m, mod[62], t[62], C)
	C, t[62] = madd2(m, mod[63], t[63], C)
	t[63], C = bits.Add64(t[64], C, 0)
	t[64], _ = bits.Add64(0, D, C)

	for j := 1; j < 64; j++ {
		//  first inner loop (second iteration)
		C, t[0] = madd1(x[j], y[0], t[0])
		C, t[1] = madd2(x[j], y[1], t[1], C)
		C, t[2] = madd2(x[j], y[2], t[2], C)
		C, t[3] = madd2(x[j], y[3], t[3], C)
		C, t[4] = madd2(x[j], y[4], t[4], C)
		C, t[5] = madd2(x[j], y[5], t[5], C)
		C, t[6] = madd2(x[j], y[6], t[6], C)
		C, t[7] = madd2(x[j], y[7], t[7], C)
		C, t[8] = madd2(x[j], y[8], t[8], C)
		C, t[9] = madd2(x[j], y[9], t[9], C)
		C, t[10] = madd2(x[j], y[10], t[10], C)
		C, t[11] = madd2(x[j], y[11], t[11], C)
		C, t[12] = madd2(x[j], y[12], t[12], C)
		C, t[13] = madd2(x[j], y[13], t[13], C)
		C, t[14] = madd2(x[j], y[14], t[14], C)
		C, t[15] = madd2(x[j], y[15], t[15], C)
		C, t[16] = madd2(x[j], y[16], t[16], C)
		C, t[17] = madd2(x[j], y[17], t[17], C)
		C, t[18] = madd2(x[j], y[18], t[18], C)
		C, t[19] = madd2(x[j], y[19], t[19], C)
		C, t[20] = madd2(x[j], y[20], t[20], C)
		C, t[21] = madd2(x[j], y[21], t[21], C)
		C, t[22] = madd2(x[j], y[22], t[22], C)
		C, t[23] = madd2(x[j], y[23], t[23], C)
		C, t[24] = madd2(x[j], y[24], t[24], C)
		C, t[25] = madd2(x[j], y[25], t[25], C)
		C, t[26] = madd2(x[j], y[26], t[26], C)
		C, t[27] = madd2(x[j], y[27], t[27], C)
		C, t[28] = madd2(x[j], y[28], t[28], C)
		C, t[29] = madd2(x[j], y[29], t[29], C)
		C, t[30] = madd2(x[j], y[30], t[30], C)
		C, t[31] = madd2(x[j], y[31], t[31], C)
		C, t[32] = madd2(x[j], y[32], t[32], C)
		C, t[33] = madd2(x[j], y[33], t[33], C)
		C, t[34] = madd2(x[j], y[34], t[34], C)
		C, t[35] = madd2(x[j], y[35], t[35], C)
		C, t[36] = madd2(x[j], y[36], t[36], C)
		C, t[37] = madd2(x[j], y[37], t[37], C)
		C, t[38] = madd2(x[j], y[38], t[38], C)
		C, t[39] = madd2(x[j], y[39], t[39], C)
		C, t[40] = madd2(x[j], y[40], t[40], C)
		C, t[41] = madd2(x[j], y[41], t[41], C)
		C, t[42] = madd2(x[j], y[42], t[42], C)
		C, t[43] = madd2(x[j], y[43], t[43], C)
		C, t[44] = madd2(x[j], y[44], t[44], C)
		C, t[45] = madd2(x[j], y[45], t[45], C)
		C, t[46] = madd2(x[j], y[46], t[46], C)
		C, t[47] = madd2(x[j], y[47], t[47], C)
		C, t[48] = madd2(x[j], y[48], t[48], C)
		C, t[49] = madd2(x[j], y[49], t[49], C)
		C, t[50] = madd2(x[j], y[50], t[50], C)
		C, t[51] = madd2(x[j], y[51], t[51], C)
		C, t[52] = madd2(x[j], y[52], t[52], C)
		C, t[53] = madd2(x[j], y[53], t[53], C)
		C, t[54] = madd2(x[j], y[54], t[54], C)
		C, t[55] = madd2(x[j], y[55], t[55], C)
		C, t[56] = madd2(x[j], y[56], t[56], C)
		C, t[57] = madd2(x[j], y[57], t[57], C)
		C, t[58] = madd2(x[j], y[58], t[58], C)
		C, t[59] = madd2(x[j], y[59], t[59], C)
		C, t[60] = madd2(x[j], y[60], t[60], C)
		C, t[61] = madd2(x[j], y[61], t[61], C)
		C, t[62] = madd2(x[j], y[62], t[62], C)
		C, t[63] = madd2(x[j], y[63], t[63], C)
		t[64], D = bits.Add64(t[64], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		C, t[0] = madd2(m, mod[1], t[1], C)
		C, t[1] = madd2(m, mod[2], t[2], C)
		C, t[2] = madd2(m, mod[3], t[3], C)
		C, t[3] = madd2(m, mod[4], t[4], C)
		C, t[4] = madd2(m, mod[5], t[5], C)
		C, t[5] = madd2(m, mod[6], t[6], C)
		C, t[6] = madd2(m, mod[7], t[7], C)
		C, t[7] = madd2(m, mod[8], t[8], C)
		C, t[8] = madd2(m, mod[9], t[9], C)
		C, t[9] = madd2(m, mod[10], t[10], C)
		C, t[10] = madd2(m, mod[11], t[11], C)
		C, t[11] = madd2(m, mod[12], t[12], C)
		C, t[12] = madd2(m, mod[13], t[13], C)
		C, t[13] = madd2(m, mod[14], t[14], C)
		C, t[14] = madd2(m, mod[15], t[15], C)
		C, t[15] = madd2(m, mod[16], t[16], C)
		C, t[16] = madd2(m, mod[17], t[17], C)
		C, t[17] = madd2(m, mod[18], t[18], C)
		C, t[18] = madd2(m, mod[19], t[19], C)
		C, t[19] = madd2(m, mod[20], t[20], C)
		C, t[20] = madd2(m, mod[21], t[21], C)
		C, t[21] = madd2(m, mod[22], t[22], C)
		C, t[22] = madd2(m, mod[23], t[23], C)
		C, t[23] = madd2(m, mod[24], t[24], C)
		C, t[24] = madd2(m, mod[25], t[25], C)
		C, t[25] = madd2(m, mod[26], t[26], C)
		C, t[26] = madd2(m, mod[27], t[27], C)
		C, t[27] = madd2(m, mod[28], t[28], C)
		C, t[28] = madd2(m, mod[29], t[29], C)
		C, t[29] = madd2(m, mod[30], t[30], C)
		C, t[30] = madd2(m, mod[31], t[31], C)
		C, t[31] = madd2(m, mod[32], t[32], C)
		C, t[32] = madd2(m, mod[33], t[33], C)
		C, t[33] = madd2(m, mod[34], t[34], C)
		C, t[34] = madd2(m, mod[35], t[35], C)
		C, t[35] = madd2(m, mod[36], t[36], C)
		C, t[36] = madd2(m, mod[37], t[37], C)
		C, t[37] = madd2(m, mod[38], t[38], C)
		C, t[38] = madd2(m, mod[39], t[39], C)
		C, t[39] = madd2(m, mod[40], t[40], C)
		C, t[40] = madd2(m, mod[41], t[41], C)
		C, t[41] = madd2(m, mod[42], t[42], C)
		C, t[42] = madd2(m, mod[43], t[43], C)
		C, t[43] = madd2(m, mod[44], t[44], C)
		C, t[44] = madd2(m, mod[45], t[45], C)
		C, t[45] = madd2(m, mod[46], t[46], C)
		C, t[46] = madd2(m, mod[47], t[47], C)
		C, t[47] = madd2(m, mod[48], t[48], C)
		C, t[48] = madd2(m, mod[49], t[49], C)
		C, t[49] = madd2(m, mod[50], t[50], C)
		C, t[50] = madd2(m, mod[51], t[51], C)
		C, t[51] = madd2(m, mod[52], t[52], C)
		C, t[52] = madd2(m, mod[53], t[53], C)
		C, t[53] = madd2(m, mod[54], t[54], C)
		C, t[54] = madd2(m, mod[55], t[55], C)
		C, t[55] = madd2(m, mod[56], t[56], C)
		C, t[56] = madd2(m, mod[57], t[57], C)
		C, t[57] = madd2(m, mod[58], t[58], C)
		C, t[58] = madd2(m, mod[59], t[59], C)
		C, t[59] = madd2(m, mod[60], t[60], C)
		C, t[60] = madd2(m, mod[61], t[61], C)
		C, t[61] = madd2(m, mod[62], t[62], C)
		C, t[62] = madd2(m, mod[63], t[63], C)
		t[63], C = bits.Add64(t[64], C, 0)
		t[64], _ = bits.Add64(0, D, C)
	}
	z[0], D = bits.Sub64(t[0], mod[0], 0)
	z[1], D = bits.Sub64(t[1], mod[1], D)
	z[2], D = bits.Sub64(t[2], mod[2], D)
	z[3], D = bits.Sub64(t[3], mod[3], D)
	z[4], D = bits.Sub64(t[4], mod[4], D)
	z[5], D = bits.Sub64(t[5], mod[5], D)
	z[6], D = bits.Sub64(t[6], mod[6], D)
	z[7], D = bits.Sub64(t[7], mod[7], D)
	z[8], D = bits.Sub64(t[8], mod[8], D)
	z[9], D = bits.Sub64(t[9], mod[9], D)
	z[10], D = bits.Sub64(t[10], mod[10], D)
	z[11], D = bits.Sub64(t[11], mod[11], D)
	z[12], D = bits.Sub64(t[12], mod[12], D)
	z[13], D = bits.Sub64(t[13], mod[13], D)
	z[14], D = bits.Sub64(t[14], mod[14], D)
	z[15], D = bits.Sub64(t[15], mod[15], D)
	z[16], D = bits.Sub64(t[16], mod[16], D)
	z[17], D = bits.Sub64(t[17], mod[17], D)
	z[18], D = bits.Sub64(t[18], mod[18], D)
	z[19], D = bits.Sub64(t[19], mod[19], D)
	z[20], D = bits.Sub64(t[20], mod[20], D)
	z[21], D = bits.Sub64(t[21], mod[21], D)
	z[22], D = bits.Sub64(t[22], mod[22], D)
	z[23], D = bits.Sub64(t[23], mod[23], D)
	z[24], D = bits.Sub64(t[24], mod[24], D)
	z[25], D = bits.Sub64(t[25], mod[25], D)
	z[26], D = bits.Sub64(t[26], mod[26], D)
	z[27], D = bits.Sub64(t[27], mod[27], D)
	z[28], D = bits.Sub64(t[28], mod[28], D)
	z[29], D = bits.Sub64(t[29], mod[29], D)
	z[30], D = bits.Sub64(t[30], mod[30], D)
	z[31], D = bits.Sub64(t[31], mod[31], D)
	z[32], D = bits.Sub64(t[32], mod[32], D)
	z[33], D = bits.Sub64(t[33], mod[33], D)
	z[34], D = bits.Sub64(t[34], mod[34], D)
	z[35], D = bits.Sub64(t[35], mod[35], D)
	z[36], D = bits.Sub64(t[36], mod[36], D)
	z[37], D = bits.Sub64(t[37], mod[37], D)
	z[38], D = bits.Sub64(t[38], mod[38], D)
	z[39], D = bits.Sub64(t[39], mod[39], D)
	z[40], D = bits.Sub64(t[40], mod[40], D)
	z[41], D = bits.Sub64(t[41], mod[41], D)
	z[42], D = bits.Sub64(t[42], mod[42], D)
	z[43], D = bits.Sub64(t[43], mod[43], D)
	z[44], D = bits.Sub64(t[44], mod[44], D)
	z[45], D = bits.Sub64(t[45], mod[45], D)
	z[46], D = bits.Sub64(t[46], mod[46], D)
	z[47], D = bits.Sub64(t[47], mod[47], D)
	z[48], D = bits.Sub64(t[48], mod[48], D)
	z[49], D = bits.Sub64(t[49], mod[49], D)
	z[50], D = bits.Sub64(t[50], mod[50], D)
	z[51], D = bits.Sub64(t[51], mod[51], D)
	z[52], D = bits.Sub64(t[52], mod[52], D)
	z[53], D = bits.Sub64(t[53], mod[53], D)
	z[54], D = bits.Sub64(t[54], mod[54], D)
	z[55], D = bits.Sub64(t[55], mod[55], D)
	z[56], D = bits.Sub64(t[56], mod[56], D)
	z[57], D = bits.Sub64(t[57], mod[57], D)
	z[58], D = bits.Sub64(t[58], mod[58], D)
	z[59], D = bits.Sub64(t[59], mod[59], D)
	z[60], D = bits.Sub64(t[60], mod[60], D)
	z[61], D = bits.Sub64(t[61], mod[61], D)
	z[62], D = bits.Sub64(t[62], mod[62], D)
	z[63], D = bits.Sub64(t[63], mod[63], D)

	if D != 0 && t[64] == 0 {
		// reduction was not necessary
		copy(z[:], t[:64])
	} /* else {
	    panic("not worst case performance")
	}*/

	return nil
}
