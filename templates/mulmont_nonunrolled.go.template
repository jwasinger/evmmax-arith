{{ $limbCount := .LimbCount}}
{{ $lastLimb := sub $limbCount 1}}
{{ $limbBits := .LimbBits}}
{{ $limbCountPlusOne := add .LimbCount 1 }}

func mulMontNonUnrolled{{mul $limbCount $limbBits}}(ctx *Field, out_bytes, x_bytes, y_bytes []byte) (error) {
	x := (*[{{$limbCount}}]uint64)(unsafe.Pointer(&x_bytes[0]))[:]
	y := (*[{{$limbCount}}]uint64)(unsafe.Pointer(&y_bytes[0]))[:]
	z := (*[{{$limbCount}}]uint64)(unsafe.Pointer(&out_bytes[0]))[:]
	mod := (*[{{$limbCount}}]uint64)(unsafe.Pointer(&ctx.Modulus[0]))[:]
	var t [{{add $limbCount 1}}]uint64
	var D uint64
	var m, C uint64

    {{ template "GTE" dict "limbCount" $limbCount "x" "x" "y" "y" "z" "mod"}}

    C, t[0] = bits.Mul64(x[j], y[0])
    {{- range $i := intRange 1 $limbCount}}
        C, t[{{$i}}] = madd1(x[j], y[{{$i}}], C)
    {{- end}}

    for j := 1; j < limbCount; j++ {
        //  first inner loop (second iteration)
        C, t[0] = madd1(x[j], y[0], t[0])
        {{- range $i := intRange 1 $limbCount }}
            C, t[{{$i}}] = madd2(x[j], y[{{$i}}], t[{{$i}}], C)
        {{- end}}
		t[{{$limbCount}}], D = bits.Add64(t[{{$limbCount}}], C, 0)
		// m = t[0]n'[0] mod W
		m = t[0] * ctx.MontParamInterleaved

		// -----------------------------------
		// Second inner loop: reduce 1 limb at a time (B**1, B**2, ...)
		C = madd0(m, mod[0], t[0])
		{{- range $i := intRange 1 $limbCount}}
				C, t[{{sub $i 1}}] = madd2(m, mod[{{$i}}], t[{{$i}}], C)
		{{- end}}
		t[{{sub $limbCount 1}}], C = bits.Add64(t[{{$limbCount}}], C, 0)
		t[{{$limbCount}}], _ = bits.Add64(0, D, C)
    }

    {{- range $i := intRange 0 $limbCount}}
        {{-  if eq $i 0 }}
            z[{{$i}}], D = bits.Sub64(t[{{$i}}], mod[{{$i}}], 0)
        {{-  else  }}
            z[{{$i}}], D = bits.Sub64(t[{{$i}}], mod[{{$i}}], D)
        {{- end}}
    {{- end}}

    if D != 0 && t[{{$limbCount}}] == 0 {
        // reduction was not necessary
        copy(z[:], t[:{{$limbCount}}])
    }/* else {
        panic("not worst case performance")
    }*/

	return nil
}
